

>>> Lint for torch/_custom_op/impl.py:

  Error (MYPY) [attr-defined]
    "type[Tag]" has no attribute "needs_fixed_stride_order"

         993  |    schema_str = f"{name}{schema}"
         994  |    function_schema = FunctionSchema.parse(schema_str)
         995  |    validate_schema(function_schema)
    >>>  996  |    tags = [torch._C.Tag.needs_fixed_stride_order] if needs_fixed_stride_order else []
         997  |    lib = library.Library(ns, "FRAGMENT")
         998  |    lib.define(schema_str, tags=tags)
         999  |    ophandle = find_ophandle_or_throw(ns, function_schema.name)



>>> Lint for torch/_decomp/decompositions.py:

  Error (MYPY) [arg-type]
    Argument 2 to "pad" has incompatible type "tuple[int, int, int, int]";
    expected "list[int]"

         914  |
         915  |    # Note that F.pad takes (padding_left, padding_right, padding_top, padding_bottom)
         916  |    # ugh
    >>>  917  |    padded_input = F.pad(input, (padding_w, padding_w, padding_h, padding_h))
         918  |
         919  |    blocks_row_indices = blocks_row_indices.unsqueeze(-1).unsqueeze(-1)
         920  |    output = padded_input[:, :, blocks_row_indices, blocks_col_indices]

  Error (MYPY) [arg-type]
    Argument 2 to "pad" has incompatible type "tuple[int, int, int, int]";
    expected "list[int]"

        1022  |    )
        1023  |    idx = (None, None, indices_row, indices_col)
        1024  |    output = aten._unsafe_index_put(output, idx, input, accumulate=True)
    >>> 1025  |    output = F.pad(output, (-padding_w, -padding_w, -padding_h, -padding_h))
        1026  |
        1027  |    if not batched_input:
        1028  |        output = output.squeeze(0)

  Error (MYPY) [return-value]
    Incompatible return value type (got "list[Tensor]", expected
    "tuple[Tensor, ...]")

        1410  |    if split_dim == 0:
        1411  |        sections = tensor_indices_or_sections.item()
        1412  |        assert isinstance(sections, IntLike)
    >>> 1413  |        return self.tensor_split(sections, dim)
        1414  |    else:
        1415  |        indices = [i.item() for i in tensor_indices_or_sections]
        1416  |        return self.tensor_split(indices, dim)

  Error (MYPY) [return-value]
    Incompatible return value type (got "list[Tensor]", expected
    "tuple[Tensor, ...]")

        1413  |        return self.tensor_split(sections, dim)
        1414  |    else:
        1415  |        indices = [i.item() for i in tensor_indices_or_sections]
    >>> 1416  |        return self.tensor_split(indices, dim)
        1417  |
        1418  |
        1419  |# TODO: this doesn't appear to have enough precision in bfloat16

  Error (MYPY) [arg-type]
    Argument "pad" to "pad" has incompatible type "tuple[int, int]"; expected
    "list[int]"

        3869  |    grid_one = torch.ones((1, 1, 1), dtype=dtype, device=device)
        3870  |
        3871  |    # this is just a temporary hack and we should use torch.stack here once #104480 is merged
    >>> 3872  |    grid_x = torch.nn.functional.pad(grid_x, pad=(0, 2), mode="constant", value=0)
        3873  |    grid_y = torch.nn.functional.pad(grid_y, pad=(1, 1), mode="constant", value=0)
        3874  |    grid_one = torch.nn.functional.pad(grid_one, pad=(2, 0), mode="constant", value=0)
        3875  |    return grid_x + grid_y + grid_one

  Error (MYPY) [arg-type]
    Argument "pad" to "pad" has incompatible type "tuple[int, int]"; expected
    "list[int]"

        3870  |
        3871  |    # this is just a temporary hack and we should use torch.stack here once #104480 is merged
        3872  |    grid_x = torch.nn.functional.pad(grid_x, pad=(0, 2), mode="constant", value=0)
    >>> 3873  |    grid_y = torch.nn.functional.pad(grid_y, pad=(1, 1), mode="constant", value=0)
        3874  |    grid_one = torch.nn.functional.pad(grid_one, pad=(2, 0), mode="constant", value=0)
        3875  |    return grid_x + grid_y + grid_one
        3876  |

  Error (MYPY) [arg-type]
    Argument "pad" to "pad" has incompatible type "tuple[int, int]"; expected
    "list[int]"

        3871  |    # this is just a temporary hack and we should use torch.stack here once #104480 is merged
        3872  |    grid_x = torch.nn.functional.pad(grid_x, pad=(0, 2), mode="constant", value=0)
        3873  |    grid_y = torch.nn.functional.pad(grid_y, pad=(1, 1), mode="constant", value=0)
    >>> 3874  |    grid_one = torch.nn.functional.pad(grid_one, pad=(2, 0), mode="constant", value=0)
        3875  |    return grid_x + grid_y + grid_one
        3876  |
        3877  |

  Error (MYPY) [arg-type]
    Argument "pad" to "pad" has incompatible type "tuple[int, int]"; expected
    "list[int]"

        3885  |    grid_one = torch.ones((1, 1, 1, 1), dtype=dtype, device=device)
        3886  |
        3887  |    # this is just a temporary hack and we should use torch.stack here once #104480 is merged
    >>> 3888  |    grid_x = torch.nn.functional.pad(grid_x, pad=(0, 3), mode="constant", value=0)
        3889  |    grid_y = torch.nn.functional.pad(grid_y, pad=(1, 2), mode="constant", value=0)
        3890  |    grid_z = torch.nn.functional.pad(grid_z, pad=(2, 1), mode="constant", value=0)
        3891  |    grid_one = torch.nn.functional.pad(grid_one, pad=(3, 0), mode="constant", value=0)

  Error (MYPY) [arg-type]
    Argument "pad" to "pad" has incompatible type "tuple[int, int]"; expected
    "list[int]"

        3886  |
        3887  |    # this is just a temporary hack and we should use torch.stack here once #104480 is merged
        3888  |    grid_x = torch.nn.functional.pad(grid_x, pad=(0, 3), mode="constant", value=0)
    >>> 3889  |    grid_y = torch.nn.functional.pad(grid_y, pad=(1, 2), mode="constant", value=0)
        3890  |    grid_z = torch.nn.functional.pad(grid_z, pad=(2, 1), mode="constant", value=0)
        3891  |    grid_one = torch.nn.functional.pad(grid_one, pad=(3, 0), mode="constant", value=0)
        3892  |    return grid_x + grid_y + grid_z + grid_one

  Error (MYPY) [arg-type]
    Argument "pad" to "pad" has incompatible type "tuple[int, int]"; expected
    "list[int]"

        3887  |    # this is just a temporary hack and we should use torch.stack here once #104480 is merged
        3888  |    grid_x = torch.nn.functional.pad(grid_x, pad=(0, 3), mode="constant", value=0)
        3889  |    grid_y = torch.nn.functional.pad(grid_y, pad=(1, 2), mode="constant", value=0)
    >>> 3890  |    grid_z = torch.nn.functional.pad(grid_z, pad=(2, 1), mode="constant", value=0)
        3891  |    grid_one = torch.nn.functional.pad(grid_one, pad=(3, 0), mode="constant", value=0)
        3892  |    return grid_x + grid_y + grid_z + grid_one
        3893  |

  Error (MYPY) [arg-type]
    Argument "pad" to "pad" has incompatible type "tuple[int, int]"; expected
    "list[int]"

        3888  |    grid_x = torch.nn.functional.pad(grid_x, pad=(0, 3), mode="constant", value=0)
        3889  |    grid_y = torch.nn.functional.pad(grid_y, pad=(1, 2), mode="constant", value=0)
        3890  |    grid_z = torch.nn.functional.pad(grid_z, pad=(2, 1), mode="constant", value=0)
    >>> 3891  |    grid_one = torch.nn.functional.pad(grid_one, pad=(3, 0), mode="constant", value=0)
        3892  |    return grid_x + grid_y + grid_z + grid_one
        3893  |
        3894  |



>>> Lint for torch/_dynamo/device_interface.py:

  Error (MYPY) [attr-defined]
    Module "torch._C" has no attribute "_xpu_getCurrentRawStream"; maybe
    "_cuda_getCurrentRawStream"?

        173  |
        174  |get_xpu_stream: Optional[Callable[[int], int]]
        175  |if torch.xpu._is_compiled():
    >>> 176  |    from torch._C import _xpu_getCurrentRawStream as get_xpu_stream
        177  |else:
        178  |    get_xpu_stream = None
        179  |

  Error (MYPY) [no-redef]
    Name "get_xpu_stream" already defined on line 174

        173  |
        174  |get_xpu_stream: Optional[Callable[[int], int]]
        175  |if torch.xpu._is_compiled():
    >>> 176  |    from torch._C import _xpu_getCurrentRawStream as get_xpu_stream
        177  |else:
        178  |    get_xpu_stream = None
        179  |



>>> Lint for torch/_export/__init__.py:

  Error (MYPY) [attr-defined]
    Module has no attribute "_aoti"

        392  |        A callable
        393  |    """
        394  |    if device == "cpu":
    >>> 395  |        runner = torch._C._aoti.AOTIModelContainerRunnerCpu(so_path, 1)  # type: ignore[call-arg]
        396  |    elif device == "cuda" or device.startswith("cuda:"):
        397  |        runner = torch._C._aoti.AOTIModelContainerRunnerCuda(so_path, 1, device)  # type: ignore[assignment, call-arg]
        398  |    else:

  Error (MYPY) [attr-defined]
    Module has no attribute "_aoti"

        394  |    if device == "cpu":
        395  |        runner = torch._C._aoti.AOTIModelContainerRunnerCpu(so_path, 1)  # type: ignore[call-arg]
        396  |    elif device == "cuda" or device.startswith("cuda:"):
    >>> 397  |        runner = torch._C._aoti.AOTIModelContainerRunnerCuda(so_path, 1, device)  # type: ignore[assignment, call-arg]
        398  |    else:
        399  |        raise RuntimeError("Unsupported device " + device)
        400  |



>>> Lint for torch/_functorch/pyfunctorch.py:

  Error (MYPY) [attr-defined]
    Module has no attribute "get_interpreter_stack"; maybe
    "peek_interpreter_stack"?

        249  |
        250  |
        251  |def retrieve_all_functorch_interpreters() -> List[FuncTorchInterpreter]:
    >>> 252  |    cis = torch._C._functorch.get_interpreter_stack()
        253  |    if cis is None:
        254  |        return []
        255  |    return [coerce_cinterpreter(ci) for ci in cis]



>>> Lint for torch/_inductor/codecache.py:

  Error (MYPY) [attr-defined]
    Module has no attribute "_aoti"

        1922  |    def convert_arg(arg):
        1923  |        if str(type(arg)) == "<class 'PyCapsule'>":
        1924  |            # No easy way to do isinstance check on PyCapsule
    >>> 1925  |            return torch._C._aoti.alloc_tensor_by_stealing_from_void_ptr(arg)
        1926  |        elif isinstance(arg, (list, tuple)):
        1927  |            return type(arg)(convert_arg(a) for a in arg)
        1928  |        else:



>>> Lint for torch/_inductor/codegen/common.py:

  Error (MYPY) [attr-defined]
    Module has no attribute "uint16"

         204  |            torch.int32,
         205  |            torch.int64,
         206  |            torch.uint8,
    >>>  207  |            torch.uint16,
         208  |            torch.uint32,
         209  |            torch.uint64,
         210  |        ]

  Error (MYPY) [attr-defined]
    Module has no attribute "uint32"

         205  |            torch.int64,
         206  |            torch.uint8,
         207  |            torch.uint16,
    >>>  208  |            torch.uint32,
         209  |            torch.uint64,
         210  |        ]
         211  |    },

  Error (MYPY) [attr-defined]
    Module has no attribute "uint64"

         206  |            torch.uint8,
         207  |            torch.uint16,
         208  |            torch.uint32,
    >>>  209  |            torch.uint64,
         210  |        ]
         211  |    },
         212  |}



>>> Lint for torch/_inductor/codegen/cpp.py:

  Error (MYPY) [attr-defined]
    Module has no attribute "uint64"

          69  |    torch.int32: "int",
          70  |    torch.int16: "short",
          71  |    torch.int8: "signed char",
    >>>   72  |    torch.uint64: "uint64_t",
          73  |    torch.uint32: "unsigned int",
          74  |    torch.uint16: "unsigned short",
          75  |    torch.uint8: "unsigned char",

  Error (MYPY) [attr-defined]
    Module has no attribute "uint32"

          70  |    torch.int16: "short",
          71  |    torch.int8: "signed char",
          72  |    torch.uint64: "uint64_t",
    >>>   73  |    torch.uint32: "unsigned int",
          74  |    torch.uint16: "unsigned short",
          75  |    torch.uint8: "unsigned char",
          76  |    torch.bool: "bool",

  Error (MYPY) [attr-defined]
    Module has no attribute "uint16"

          71  |    torch.int8: "signed char",
          72  |    torch.uint64: "uint64_t",
          73  |    torch.uint32: "unsigned int",
    >>>   74  |    torch.uint16: "unsigned short",
          75  |    torch.uint8: "unsigned char",
          76  |    torch.bool: "bool",
          77  |    torch.bfloat16: "bfloat16",

  Error (MYPY) [attr-defined]
    Module has no attribute "uint64"

          88  |    torch.int32: "at::kInt",
          89  |    torch.int16: "at::kShort",
          90  |    torch.int8: "at::kChar",
    >>>   91  |    torch.uint64: "at::kUInt64",
          92  |    torch.uint32: "at::kUInt32",
          93  |    torch.uint16: "at::kUInt16",
          94  |    torch.uint8: "at::kByte",

  Error (MYPY) [attr-defined]
    Module has no attribute "uint32"

          89  |    torch.int16: "at::kShort",
          90  |    torch.int8: "at::kChar",
          91  |    torch.uint64: "at::kUInt64",
    >>>   92  |    torch.uint32: "at::kUInt32",
          93  |    torch.uint16: "at::kUInt16",
          94  |    torch.uint8: "at::kByte",
          95  |    torch.uint32: "at::kUInt32",

  Error (MYPY) [attr-defined]
    Module has no attribute "uint16"

          90  |    torch.int8: "at::kChar",
          91  |    torch.uint64: "at::kUInt64",
          92  |    torch.uint32: "at::kUInt32",
    >>>   93  |    torch.uint16: "at::kUInt16",
          94  |    torch.uint8: "at::kByte",
          95  |    torch.uint32: "at::kUInt32",
          96  |    torch.uint64: "at::kUInt64",

  Error (MYPY) [attr-defined]
    Module has no attribute "uint32"

          92  |    torch.uint32: "at::kUInt32",
          93  |    torch.uint16: "at::kUInt16",
          94  |    torch.uint8: "at::kByte",
    >>>   95  |    torch.uint32: "at::kUInt32",
          96  |    torch.uint64: "at::kUInt64",
          97  |    torch.bool: "at::kBool",
          98  |    torch.bfloat16: "at::kBFloat16",

  Error (MYPY) [attr-defined]
    Module has no attribute "uint64"

          93  |    torch.uint16: "at::kUInt16",
          94  |    torch.uint8: "at::kByte",
          95  |    torch.uint32: "at::kUInt32",
    >>>   96  |    torch.uint64: "at::kUInt64",
          97  |    torch.bool: "at::kBool",
          98  |    torch.bfloat16: "at::kBFloat16",
          99  |    torch.complex32: "at::kComplexHalf",



>>> Lint for torch/_inductor/cudagraph_trees.py:

  Error (MYPY) [call-arg]
    Too many arguments for "_cuda_endAllocateCurrentStreamToPool"

         517  |        try:
         518  |            yield
         519  |        finally:
    >>>  520  |            torch._C._cuda_endAllocateCurrentStreamToPool(device, mem_pool)
         521  |            torch._C._cuda_releasePool(device, mem_pool)
         522  |
         523  |    torch.cuda.current_stream().wait_stream(stream)

  Error (MYPY) [attr-defined]
    Module has no attribute "_tensors_data_ptrs_at_indices_equal"

         942  |
         943  |    def check_static_inputs_are_stable(self, new_inputs):
         944  |        # avoid checking managed tensor static points since we already checked those in check_invariants
    >>>  945  |        if not torch._C._tensors_data_ptrs_at_indices_equal(
         946  |            new_inputs, self.static_input_data_ptrs, self.non_managed_static_input_idxs
         947  |        ):
         948  |            # this should error

  Error (MYPY) [attr-defined]
    Module has no attribute "_tensors_data_ptrs_at_indices_equal"

        1536  |        # previously managed data pointers remain stable
        1537  |        # this is on the hot path so moved to C++. equivalent to:
        1538  |        # return all(t.data_ptr() == data_ptr for (t, data_ptr) in zip(tensors, data_ptrs))
    >>> 1539  |        if not torch._C._tensors_data_ptrs_at_indices_equal(
        1540  |            inputs, self.static_input_data_ptrs, self.cudagraph_managed_idxs
        1541  |        ):
        1542  |            return False



>>> Lint for torch/_inductor/fx_passes/fb/fuse_split_ops.py:

  Error (MYPY) [no-redef]
    Name "total_node_dims" already defined on line 718

        720  |            arg_dim: int = node.kwargs["dim"]  # type: ignore[assignment]
        721  |            return (total_node_dims + arg_dim) % total_node_dims
        722  |        elif is_op(node, torch.ops.aten.chunk):
    >>> 723  |            total_node_dims: int = len(
        724  |                node.all_input_nodes[0].meta["tensor_meta"].shape
        725  |            )
        726  |            # pyre-ignore

  Error (MYPY) [return-value]
    Incompatible return value type (got "tuple[Any, ...] | list[Any] |
    dict[str, Any] | slice | range | Node | str | int | float | bool | complex
    | dtype | Tensor | device | memory_format | layout | OpOverload | None",
    expected "int | None")

        727  |            arg_dim: int = node.kwargs["dim"]  # type: ignore[no-redef]
        728  |            return (total_node_dims + arg_dim) % total_node_dims
        729  |        elif is_op(node, torch.ops.aten.split_with_sizes):
    >>> 730  |            return node.kwargs["dim"]
        731  |    return None
        732  |
        733  |

  Error (MYPY) [assignment]
    Incompatible types in assignment (expression has type "int | None",
    variable has type "int")

        776  |                break
        777  |
        778  |            # pyre-ignore
    >>> 779  |            normalized_real_user_dim: int = get_normalized_dim_kwarg(real_user)
        780  |            if normalized_real_user_dim != normalized_node_dim:
        781  |                can_remove = False
        782  |                break

  Error (MYPY) [union-attr]
    Item "None" of "Node | None" has no attribute "kwargs"

        793  |
        794  |        # chunk has to be in exact order as concat.
        795  |        pos: Optional[int] = None
    >>> 796  |        org_inputs: List[fx.Node] = cat_node.kwargs["tensors"]
        797  |        for i, _ in enumerate(org_inputs):
        798  |            if split_op.getitems == org_inputs[i : i + len(split_op.getitems)]:
        799  |                pos = i

  Error (MYPY) [assignment]
    Incompatible types in assignment (expression has type "tuple[Any, ...] |
    list[Any] | dict[str, Any] | slice | range | Node | str | int | float |
    complex | dtype | Tensor | device | memory_format | layout | OpOverload |
    Any | None", variable has type "list[Node]")

        793  |
        794  |        # chunk has to be in exact order as concat.
        795  |        pos: Optional[int] = None
    >>> 796  |        org_inputs: List[fx.Node] = cat_node.kwargs["tensors"]
        797  |        for i, _ in enumerate(org_inputs):
        798  |            if split_op.getitems == org_inputs[i : i + len(split_op.getitems)]:
        799  |                pos = i

  Error (MYPY) [union-attr]
    Item "None" of "Node | None" has no attribute "kwargs"

        809  |                + [split_op.pre_split_header]
        810  |                + org_inputs[pos + len(split_op.getitems) :]
        811  |            ),
    >>> 812  |            "dim": cat_node.kwargs["dim"],
        813  |        }
        814  |        cat_node.kwargs = kwargs
        815  |        removed = True

  Error (MYPY) [union-attr]
    Item "None" of "Node | None" has no attribute "kwargs"

        811  |            ),
        812  |            "dim": cat_node.kwargs["dim"],
        813  |        }
    >>> 814  |        cat_node.kwargs = kwargs
        815  |        removed = True
        816  |
        817  |    if removed:



>>> Lint for torch/_inductor/fx_passes/fb/remove_reshape.py:

  Error (MYPY) [union-attr]
    Item "tuple[Any, ...]" of "tuple[Any, ...] | Any | list[Any] | dict[str,
    Any] | slice | range | Node | str | int | float | complex | dtype | Tensor
    | device | memory_format | layout | OpOverload | None" has no attribute
    "format_node"

        131  |                input_node, torch.ops.aten.clone
        132  |            ):
        133  |                input_node = input_node.kwargs["self"]
    >>> 134  |            _LOGGER.info(f"replacing {n.format_node()} with {input_node.format_node()}")
        135  |            n.replace_all_uses_with(input_node)
        136  |
        137  |    graph.eliminate_dead_code()

  Error (MYPY) [union-attr]
    Item "list[Any]" of "tuple[Any, ...] | Any | list[Any] | dict[str, Any]
    | slice | range | Node | str | int | float | complex | dtype | Tensor
    | device | memory_format | layout | OpOverload | None" has no attribute
    "format_node"

        131  |                input_node, torch.ops.aten.clone
        132  |            ):
        133  |                input_node = input_node.kwargs["self"]
    >>> 134  |            _LOGGER.info(f"replacing {n.format_node()} with {input_node.format_node()}")
        135  |            n.replace_all_uses_with(input_node)
        136  |
        137  |    graph.eliminate_dead_code()

  Error (MYPY) [union-attr]
    Item "dict[str, Any]" of "tuple[Any, ...] | Any | list[Any] | dict[str,
    Any] | slice | range | Node | str | int | float | complex | dtype | Tensor
    | device | memory_format | layout | OpOverload | None" has no attribute
    "format_node"

        131  |                input_node, torch.ops.aten.clone
        132  |            ):
        133  |                input_node = input_node.kwargs["self"]
    >>> 134  |            _LOGGER.info(f"replacing {n.format_node()} with {input_node.format_node()}")
        135  |            n.replace_all_uses_with(input_node)
        136  |
        137  |    graph.eliminate_dead_code()

  Error (MYPY) [union-attr]
    Item "slice" of "tuple[Any, ...] | Any | list[Any] | dict[str, Any] |
    slice | range | Node | str | int | float | complex | dtype | Tensor |
    device | memory_format | layout | OpOverload | None" has no attribute
    "format_node"

        131  |                input_node, torch.ops.aten.clone
        132  |            ):
        133  |                input_node = input_node.kwargs["self"]
    >>> 134  |            _LOGGER.info(f"replacing {n.format_node()} with {input_node.format_node()}")
        135  |            n.replace_all_uses_with(input_node)
        136  |
        137  |    graph.eliminate_dead_code()

  Error (MYPY) [union-attr]
    Item "range" of "tuple[Any, ...] | Any | list[Any] | dict[str, Any] |
    slice | range | Node | str | int | float | complex | dtype | Tensor |
    device | memory_format | layout | OpOverload | None" has no attribute
    "format_node"

        131  |                input_node, torch.ops.aten.clone
        132  |            ):
        133  |                input_node = input_node.kwargs["self"]
    >>> 134  |            _LOGGER.info(f"replacing {n.format_node()} with {input_node.format_node()}")
        135  |            n.replace_all_uses_with(input_node)
        136  |
        137  |    graph.eliminate_dead_code()

  Error (MYPY) [union-attr]
    Item "str" of "tuple[Any, ...] | Any | list[Any] | dict[str, Any] | slice
    | range | Node | str | int | float | complex | dtype | Tensor | device |
    memory_format | layout | OpOverload | None" has no attribute "format_node"

        131  |                input_node, torch.ops.aten.clone
        132  |            ):
        133  |                input_node = input_node.kwargs["self"]
    >>> 134  |            _LOGGER.info(f"replacing {n.format_node()} with {input_node.format_node()}")
        135  |            n.replace_all_uses_with(input_node)
        136  |
        137  |    graph.eliminate_dead_code()

  Error (MYPY) [union-attr]
    Item "int" of "tuple[Any, ...] | Any | list[Any] | dict[str, Any] | slice
    | range | Node | str | int | float | complex | dtype | Tensor | device |
    memory_format | layout | OpOverload | None" has no attribute "format_node"

        131  |                input_node, torch.ops.aten.clone
        132  |            ):
        133  |                input_node = input_node.kwargs["self"]
    >>> 134  |            _LOGGER.info(f"replacing {n.format_node()} with {input_node.format_node()}")
        135  |            n.replace_all_uses_with(input_node)
        136  |
        137  |    graph.eliminate_dead_code()

  Error (MYPY) [union-attr]
    Item "float" of "tuple[Any, ...] | Any | list[Any] | dict[str, Any] |
    slice | range | Node | str | int | float | complex | dtype | Tensor |
    device | memory_format | layout | OpOverload | None" has no attribute
    "format_node"

        131  |                input_node, torch.ops.aten.clone
        132  |            ):
        133  |                input_node = input_node.kwargs["self"]
    >>> 134  |            _LOGGER.info(f"replacing {n.format_node()} with {input_node.format_node()}")
        135  |            n.replace_all_uses_with(input_node)
        136  |
        137  |    graph.eliminate_dead_code()

  Error (MYPY) [union-attr]
    Item "complex" of "tuple[Any, ...] | Any | list[Any] | dict[str, Any]
    | slice | range | Node | str | int | float | complex | dtype | Tensor
    | device | memory_format | layout | OpOverload | None" has no attribute
    "format_node"

        131  |                input_node, torch.ops.aten.clone
        132  |            ):
        133  |                input_node = input_node.kwargs["self"]
    >>> 134  |            _LOGGER.info(f"replacing {n.format_node()} with {input_node.format_node()}")
        135  |            n.replace_all_uses_with(input_node)
        136  |
        137  |    graph.eliminate_dead_code()

  Error (MYPY) [union-attr]
    Item "dtype" of "tuple[Any, ...] | Any | list[Any] | dict[str, Any] |
    slice | range | Node | str | int | float | complex | dtype | Tensor |
    device | memory_format | layout | OpOverload | None" has no attribute
    "format_node"

        131  |                input_node, torch.ops.aten.clone
        132  |            ):
        133  |                input_node = input_node.kwargs["self"]
    >>> 134  |            _LOGGER.info(f"replacing {n.format_node()} with {input_node.format_node()}")
        135  |            n.replace_all_uses_with(input_node)
        136  |
        137  |    graph.eliminate_dead_code()

  Error (MYPY) [union-attr]
    Item "Tensor" of "tuple[Any, ...] | Any | list[Any] | dict[str, Any] |
    slice | range | Node | str | int | float | complex | dtype | Tensor |
    device | memory_format | layout | OpOverload | None" has no attribute
    "format_node"

        131  |                input_node, torch.ops.aten.clone
        132  |            ):
        133  |                input_node = input_node.kwargs["self"]
    >>> 134  |            _LOGGER.info(f"replacing {n.format_node()} with {input_node.format_node()}")
        135  |            n.replace_all_uses_with(input_node)
        136  |
        137  |    graph.eliminate_dead_code()

  Error (MYPY) [union-attr]
    Item "device" of "tuple[Any, ...] | Any | list[Any] | dict[str, Any] |
    slice | range | Node | str | int | float | complex | dtype | Tensor |
    device | memory_format | layout | OpOverload | None" has no attribute
    "format_node"

        131  |                input_node, torch.ops.aten.clone
        132  |            ):
        133  |                input_node = input_node.kwargs["self"]
    >>> 134  |            _LOGGER.info(f"replacing {n.format_node()} with {input_node.format_node()}")
        135  |            n.replace_all_uses_with(input_node)
        136  |
        137  |    graph.eliminate_dead_code()

  Error (MYPY) [union-attr]
    Item "memory_format" of "tuple[Any, ...] | Any | list[Any] | dict[str,
    Any] | slice | range | Node | str | int | float | complex | dtype | Tensor
    | device | memory_format | layout | OpOverload | None" has no attribute
    "format_node"

        131  |                input_node, torch.ops.aten.clone
        132  |            ):
        133  |                input_node = input_node.kwargs["self"]
    >>> 134  |            _LOGGER.info(f"replacing {n.format_node()} with {input_node.format_node()}")
        135  |            n.replace_all_uses_with(input_node)
        136  |
        137  |    graph.eliminate_dead_code()

  Error (MYPY) [union-attr]
    Item "layout" of "tuple[Any, ...] | Any | list[Any] | dict[str, Any] |
    slice | range | Node | str | int | float | complex | dtype | Tensor |
    device | memory_format | layout | OpOverload | None" has no attribute
    "format_node"

        131  |                input_node, torch.ops.aten.clone
        132  |            ):
        133  |                input_node = input_node.kwargs["self"]
    >>> 134  |            _LOGGER.info(f"replacing {n.format_node()} with {input_node.format_node()}")
        135  |            n.replace_all_uses_with(input_node)
        136  |
        137  |    graph.eliminate_dead_code()

  Error (MYPY) [union-attr]
    Item "OpOverload" of "tuple[Any, ...] | Any | list[Any] | dict[str, Any]
    | slice | range | Node | str | int | float | complex | dtype | Tensor
    | device | memory_format | layout | OpOverload | None" has no attribute
    "format_node"

        131  |                input_node, torch.ops.aten.clone
        132  |            ):
        133  |                input_node = input_node.kwargs["self"]
    >>> 134  |            _LOGGER.info(f"replacing {n.format_node()} with {input_node.format_node()}")
        135  |            n.replace_all_uses_with(input_node)
        136  |
        137  |    graph.eliminate_dead_code()

  Error (MYPY) [union-attr]
    Item "None" of "tuple[Any, ...] | Any | list[Any] | dict[str, Any] | slice
    | range | Node | str | int | float | complex | dtype | Tensor | device |
    memory_format | layout | OpOverload | None" has no attribute "format_node"

        131  |                input_node, torch.ops.aten.clone
        132  |            ):
        133  |                input_node = input_node.kwargs["self"]
    >>> 134  |            _LOGGER.info(f"replacing {n.format_node()} with {input_node.format_node()}")
        135  |            n.replace_all_uses_with(input_node)
        136  |
        137  |    graph.eliminate_dead_code()



>>> Lint for torch/_inductor/fx_passes/fb/split_cat_aten.py:

  Error (MYPY) [union-attr]
    Item "str" of "Any | str | int" has no attribute "args"

        1283  |            indices, idx_to_getitem = [], {}
        1284  |            for getitem in cat_user.args[0]:  # type: ignore[union-attr]
        1285  |                indices.append(getitem.args[1])  # type: ignore[union-attr]
    >>> 1286  |                idx_to_getitem[getitem.args[1]] = getitem  # type: ignore[index]
        1287  |            # the gettitems to be merged must be consecutive, otherwise
        1288  |            # returned sliced tensor could be wrong
        1289  |            if not is_sorted_and_consecutive(indices):

  Error (MYPY) [union-attr]
    Item "int" of "Any | str | int" has no attribute "args"

        1283  |            indices, idx_to_getitem = [], {}
        1284  |            for getitem in cat_user.args[0]:  # type: ignore[union-attr]
        1285  |                indices.append(getitem.args[1])  # type: ignore[union-attr]
    >>> 1286  |                idx_to_getitem[getitem.args[1]] = getitem  # type: ignore[index]
        1287  |            # the gettitems to be merged must be consecutive, otherwise
        1288  |            # returned sliced tensor could be wrong
        1289  |            if not is_sorted_and_consecutive(indices):

  Error (MYPY) [arg-type]
    Argument 1 to "len" has incompatible type "tuple[Any, ...] | list[Any] |
    dict[str, Any] | slice | range | Node | str | int | float | bool | complex
    | dtype | Tensor | device | memory_format | layout | OpOverload | None";
    expected "Sized"

        1289  |            if not is_sorted_and_consecutive(indices):
        1290  |                continue
        1291  |            # case 1: the cat uses all getitems from the split
    >>> 1292  |            if len(split_sections) == len(cat_user.args[0]):  # type: ignore[union-attr]
        1293  |                # replace the users of the cat node to be the input of the split node
        1294  |                cat_user.replace_all_uses_with(split_node.args[0])
        1295  |                # remove the cat node

  Error (MYPY) [arg-type]
    Argument 1 to "is_node_meta_valid" has incompatible type "tuple[Any, ...]
    | list[Any] | dict[str, Any] | slice | range | Node | str | int | float
    | bool | complex | dtype | Tensor | device | memory_format | layout |
    OpOverload | None"; expected "Node | None"

        1296  |                graph.erase_node(cat_user)
        1297  |                counters["inductor"]["mutate_cat_pass"] += 1
        1298  |            # case 2: the cat uses some getitems from the split
    >>> 1299  |            elif is_node_meta_valid(split_node.args[0]):  # type: ignore[union-attr]
        1300  |                # check the split dim, and construct the slice tuple
        1301  |                start_fused_size = calculate_fused_tensor_size(
        1302  |                    split_node, list(range(indices[0]))



>>> Lint for torch/_inductor/graph.py:

  Error (MYPY) [attr-defined]
    "type[Tag]" has no attribute "needs_fixed_stride_order"

         805  |            # which run through implicit fallback must constrain their
         806  |            # arguments' fx strides
         807  |            layout_constraint = None
    >>>  808  |            if torch._C.Tag.needs_fixed_stride_order in target.tags:
         809  |                # We have to set the current args because call_function will immediately
         810  |                # evaluate this lowering after creating the fallback, without evaluating
         811  |                # the layout constraint



>>> Lint for torch/_inductor/triton_heuristics.py:

  Error (MYPY) [call-arg]
    Too many arguments for "_RecordFunctionFast"

         733  |        # it is faster than entering and exiting a context manager, even if the context
         734  |        # manager is a nullcontext.
         735  |        if autograd_profiler._is_profiler_enabled:
    >>>  736  |            with torch._C._profiler._RecordFunctionFast(
         737  |                self.inductor_meta.get("kernel_name", "triton kernel"), args
         738  |            ):
         739  |                return launcher(



>>> Lint for torch/_library/custom_ops.py:

  Error (MYPY) [attr-defined]
    Module has no attribute "_AutoDispatchBelowADInplaceOrView"

        418  |                        for v in val:
        419  |                            if isinstance(v, Tensor):
        420  |                                autograd.graph.increment_version(v)
    >>> 421  |                with _C._AutoDispatchBelowADInplaceOrView():
        422  |                    return self._opoverload(*args, **kwargs)
        423  |
        424  |            lib.impl(self._name, adinplaceorview_impl, "ADInplaceOrView")



>>> Lint for torch/_ops.py:

  Error (MYPY) [attr-defined]
    "type[DispatchKey]" has no attribute "PreDispatch"

         313  |                return handler(mode, *args, **kwargs)
         314  |
         315  |        functionality_key = torch._C._to_functionality_key(dispatch_key)  # type: ignore[attr-defined]
    >>>  316  |        if functionality_key == torch._C.DispatchKey.PreDispatch:
         317  |            from torch.utils._python_dispatch import _pop_mode_temporarily
         318  |
         319  |            # The check for Python in the exclude set is so we properly respect `with no_dispatch()`

  Error (MYPY) [attr-defined]
    "type[DispatchKey]" has no attribute "PreDispatch"

         348  |        # on what modes are active, predispatch behaviour is different.
         349  |        # Also we do same thing for normal ops:
         350  |        # See Note [Not Caching Per-Dispatch-Key Mode Handlers]
    >>>  351  |        if dispatch_key != torch._C.DispatchKey.PreDispatch:
         352  |            self._dispatch_cache[dispatch_key] = self.py_kernels[final_key]
         353  |        kernel = self.py_kernels[final_key]
         354  |        # It's illegal to register DispatchKey to py_kernels, since there's no

  Error (MYPY) [attr-defined]
    "type[DispatchKey]" has no attribute "PreDispatch"

         461  |    # set.
         462  |    if new_pre_dispatch_len == 0:
         463  |        torch._C._dispatch_tls_set_dispatch_key_included(
    >>>  464  |            torch._C.DispatchKey.PreDispatch, False
         465  |        )
         466  |
         467  |    return current_mode

  Error (MYPY) [attr-defined]
    "type[DispatchKey]" has no attribute "PreDispatch"

         489  |    # was turned off. So we need to turn it on again.
         490  |    if previous_mode_stack_len == 0:
         491  |        torch._C._dispatch_tls_set_dispatch_key_included(
    >>>  492  |            torch._C.DispatchKey.PreDispatch, True
         493  |        )
         494  |
         495  |

  Error (MYPY) [attr-defined]
    "type[DispatchKey]" has no attribute "PreDispatch"

         691  |            return handler
         692  |
         693  |        functionality_key = torch._C._to_functionality_key(key)  # type: ignore[attr-defined]
    >>>  694  |        if functionality_key == torch._C.DispatchKey.PreDispatch:
         695  |            curr_stack_len = _len_torch_dispatch_stack_pre_dispatch()
         696  |            # The check for Python in the exclude set is so we properly respect `with no_dispatch()`
         697  |            # calls inside of a mode.

  Error (MYPY) [attr-defined]
    "type[DispatchKey]" has no attribute "PreDispatch"

         742  |        final_key = resolve_key(self, key)
         743  |
         744  |        # See Note [Not Caching Per-Dispatch-Key Mode Handlers]
    >>>  745  |        cache_result = key != torch._C.DispatchKey.PreDispatch
         746  |
         747  |        # TODO: We could potentially have lots of debugging wrappers against
         748  |        # dispatch keys; design some general registration mechanism instead of



>>> Lint for torch/_prims_common/__init__.py:

  Error (MYPY) [attr-defined]
    Module has no attribute "uint16"

         932  |
         933  |_integer_dtypes = (
         934  |    torch.uint8,
    >>>  935  |    torch.uint16,
         936  |    torch.uint32,
         937  |    torch.uint64,
         938  |    torch.int8,

  Error (MYPY) [attr-defined]
    Module has no attribute "uint32"

         933  |_integer_dtypes = (
         934  |    torch.uint8,
         935  |    torch.uint16,
    >>>  936  |    torch.uint32,
         937  |    torch.uint64,
         938  |    torch.int8,
         939  |    torch.int16,

  Error (MYPY) [attr-defined]
    Module has no attribute "uint64"

         934  |    torch.uint8,
         935  |    torch.uint16,
         936  |    torch.uint32,
    >>>  937  |    torch.uint64,
         938  |    torch.int8,
         939  |    torch.int16,
         940  |    torch.int32,



>>> Lint for torch/_refs/__init__.py:

  Error (MYPY) [attr-defined]
    Module has no attribute "frexp"

        1371  |@register_decomposition(aten.frexp)
        1372  |@out_wrapper("mantissa", "exponent")
        1373  |def frexp(self: TensorLikeType) -> Tuple[TensorLikeType, TensorLikeType]:
    >>> 1374  |    return torch.return_types.frexp(prims.frexp(self))
        1375  |
        1376  |
        1377  |@_make_elementwise_binary_reference(

  Error (MYPY) [arg-type]
    Argument 1 to "sub" has incompatible type "Tensor | bool | int | float |
    complex"; expected "Tensor | int | float | bool"

        1689  |        msg = "Received a Number for the first argument, but expected a Tensor"
        1690  |        raise ValueError(msg)
        1691  |
    >>> 1692  |    return torch.sub(b, a, alpha=alpha)
        1693  |
        1694  |
        1695  |# TODO: consider refactoring this with add impl

  Error (MYPY) [arg-type]
    Argument "alpha" to "sub" has incompatible type "bool | int | float |
    complex"; expected "int | float | bool | None"

        1689  |        msg = "Received a Number for the first argument, but expected a Tensor"
        1690  |        raise ValueError(msg)
        1691  |
    >>> 1692  |    return torch.sub(b, a, alpha=alpha)
        1693  |
        1694  |
        1695  |# TODO: consider refactoring this with add impl



>>> Lint for torch/_subclasses/functional_tensor.py:

  Error (MYPY) [attr-defined]
    Module has no attribute "_set_throw_on_mutable_data_ptr"

        120  |            False,  # dispatch_layout
        121  |            extra_dispatch_keys,  # _extra_dispatch_keys
        122  |        )
    >>> 123  |        torch._C._set_throw_on_mutable_data_ptr(out)
        124  |        out.elem = elem
        125  |        return out
        126  |

  Error (MYPY) [attr-defined]
    "type[DispatchKey]" has no attribute "PreDispatch"

        246  |    # No-op if FunctionalTensorMode is already in use
        247  |    def __enter__(self):
        248  |        def _get_prev_mode():
    >>> 249  |            if self._dispatch_key == torch._C.DispatchKey.PreDispatch:
        250  |                return _get_dispatch_mode_pre_dispatch(
        251  |                    torch._C._TorchDispatchModeKey.FUNCTIONAL
        252  |                )



>>> Lint for torch/_subclasses/meta_utils.py:

  Error (MYPY) [attr-defined]
    Module "torch._C._functorch" has no attribute "is_legacy_batchedtensor";
    maybe "is_batchedtensor"?

          21  |from typing_extensions import TypeAlias
          22  |
          23  |import torch
    >>>   24  |from torch._C._functorch import (
          25  |    _add_batch_dim,
          26  |    _unwrap_functional_tensor,
          27  |    _wrap_functional_tensor,

  Error (MYPY) [attr-defined]
    "Tensor" has no attribute "_view_func_unsafe"

         344  |            if recurse and t._is_view() and t._base is not None
         345  |            else None,
         346  |            fake_mode=torch._subclasses.fake_tensor.maybe_get_fake_mode(t),
    >>>  347  |            view_func=t._view_func_unsafe,
         348  |            attrs=attrs,
         349  |            ctx=ctx,
         350  |            type=type_v,



>>> Lint for torch/_tensor.py:

  Error (MYPY) [attr-defined]
    Module has no attribute "split_with_sizes"

         927  |        if isinstance(split_size, (int, torch.SymInt)):
         928  |            return torch._VF.split(self, split_size, dim)  # type: ignore[attr-defined]
         929  |        else:
    >>>  930  |            return torch._VF.split_with_sizes(self, split_size, dim)
         931  |
         932  |    def unique(self, sorted=True, return_inverse=False, return_counts=False, dim=None):
         933  |        r"""Returns the unique elements of the input tensor.



>>> Lint for torch/_torch_docs.py:

  Error (MYPY) [attr-defined]
    "type[Generator]" has no attribute "graphsafe_set_state"

        13723  |)
        13724  |
        13725  |add_docstr(
    >>> 13726  |    torch.Generator.graphsafe_set_state,
        13727  |    r"""
        13728  |Generator.graphsafe_set_state(state) -> None
        13729  |

  Error (MYPY) [attr-defined]
    "type[Generator]" has no attribute "graphsafe_get_state"

        13742  |)
        13743  |
        13744  |add_docstr(
    >>> 13745  |    torch.Generator.graphsafe_get_state,
        13746  |    r"""
        13747  |Generator.graphsafe_get_state() -> torch.Generator
        13748  |

  Error (MYPY) [attr-defined]
    "type[Generator]" has no attribute "clone_state"

        13759  |)
        13760  |
        13761  |add_docstr(
    >>> 13762  |    torch.Generator.clone_state,
        13763  |    r"""
        13764  |Generator.clone_state() -> torch.Generator
        13765  |



>>> Lint for torch/ao/nn/quantizable/modules/activation.py:

  Error (MYPY) [arg-type]
    Argument 2 to "pad" has incompatible type "tuple[int, int]"; expected
    "list[int]"

        369  |                k = torch.cat([k, bias_k.repeat(1, bsz, 1)])
        370  |                v = torch.cat([v, bias_v.repeat(1, bsz, 1)])
        371  |                if attn_mask is not None:
    >>> 372  |                    attn_mask = nnF.pad(attn_mask, (0, 1))
        373  |                if key_padding_mask is not None:
        374  |                    key_padding_mask = nnF.pad(key_padding_mask, (0, 1))
        375  |            else:

  Error (MYPY) [arg-type]
    Argument 2 to "pad" has incompatible type "tuple[int, int]"; expected
    "list[int]"

        371  |                if attn_mask is not None:
        372  |                    attn_mask = nnF.pad(attn_mask, (0, 1))
        373  |                if key_padding_mask is not None:
    >>> 374  |                    key_padding_mask = nnF.pad(key_padding_mask, (0, 1))
        375  |            else:
        376  |                assert static_k is None, "bias cannot be added to static key."
        377  |                assert static_v is None, "bias cannot be added to static value."

  Error (MYPY) [arg-type]
    Argument 2 to "pad" has incompatible type "tuple[int, int]"; expected
    "list[int]"

        413  |            v = torch.cat([v, v_zeros], dim=1)
        414  |
        415  |            if attn_mask is not None:
    >>> 416  |                attn_mask = nnF.pad(attn_mask, (0, 1))
        417  |            if key_padding_mask is not None:
        418  |                key_padding_mask = nnF.pad(key_padding_mask, (0, 1))
        419  |

  Error (MYPY) [arg-type]
    Argument 2 to "pad" has incompatible type "tuple[int, int]"; expected
    "list[int]"

        415  |            if attn_mask is not None:
        416  |                attn_mask = nnF.pad(attn_mask, (0, 1))
        417  |            if key_padding_mask is not None:
    >>> 418  |                key_padding_mask = nnF.pad(key_padding_mask, (0, 1))
        419  |
        420  |        # Leaving the quantized zone here
        421  |        q = self.dequant_q(q)



>>> Lint for torch/ao/nn/quantized/reference/modules/rnn.py:

  Error (MYPY) [attr-defined]
    Module has no attribute "rnn_tanh_cell"

        138  |            hx = hx.unsqueeze(0) if not is_batched else hx
        139  |
        140  |        if self.nonlinearity == "tanh":
    >>> 141  |            ret = _VF.rnn_tanh_cell(
        142  |                input, hx,
        143  |                self.get_weight_ih(), self.get_weight_hh(),
        144  |                self.bias_ih, self.bias_hh,

  Error (MYPY) [attr-defined]
    Module has no attribute "rnn_relu_cell"

        144  |                self.bias_ih, self.bias_hh,
        145  |            )
        146  |        elif self.nonlinearity == "relu":
    >>> 147  |            ret = _VF.rnn_relu_cell(
        148  |                input, hx,
        149  |                self.get_weight_ih(), self.get_weight_hh(),
        150  |                self.bias_ih, self.bias_hh,

  Error (MYPY) [attr-defined]
    Module has no attribute "lstm_cell"

        202  |        else:
        203  |            hx = (hx[0].unsqueeze(0), hx[1].unsqueeze(0)) if not is_batched else hx
        204  |
    >>> 205  |        ret = _VF.lstm_cell(
        206  |            input, hx,
        207  |            self.get_weight_ih(), self.get_weight_hh(),
        208  |            self.bias_ih, self.bias_hh,

  Error (MYPY) [attr-defined]
    Module has no attribute "gru_cell"

        253  |        else:
        254  |            hx = hx.unsqueeze(0) if not is_batched else hx
        255  |
    >>> 256  |        ret = _VF.gru_cell(
        257  |            input, hx,
        258  |            self.get_weight_ih(), self.get_weight_hh(),
        259  |            self.bias_ih, self.bias_hh,

  Error (MYPY) [attr-defined]
    Module has no attribute "lstm"

        453  |
        454  |        self.check_forward_args(input, hx, batch_sizes)
        455  |        if batch_sizes is None:
    >>> 456  |            result = _VF.lstm(input, hx, self.get_flat_weights(), self.bias, self.num_layers,
        457  |                              self.dropout, self.training, self.bidirectional, self.batch_first)
        458  |        else:
        459  |            result = _VF.lstm(input, batch_sizes, hx, self.get_flat_weights(), self.bias,

  Error (MYPY) [attr-defined]
    Module has no attribute "lstm"

        456  |            result = _VF.lstm(input, hx, self.get_flat_weights(), self.bias, self.num_layers,
        457  |                              self.dropout, self.training, self.bidirectional, self.batch_first)
        458  |        else:
    >>> 459  |            result = _VF.lstm(input, batch_sizes, hx, self.get_flat_weights(), self.bias,
        460  |                              self.num_layers, self.dropout, self.training, self.bidirectional)
        461  |        output = result[0]
        462  |        hidden = result[1:]

  Error (MYPY) [attr-defined]
    Module has no attribute "gru"

        576  |
        577  |        self.check_forward_args(input, hx, batch_sizes)
        578  |        if batch_sizes is None:
    >>> 579  |            result = _VF.gru(input, hx, self.get_flat_weights(), self.bias, self.num_layers,
        580  |                             self.dropout, self.training, self.bidirectional, self.batch_first)
        581  |        else:
        582  |            result = _VF.gru(input, batch_sizes, hx, self.get_flat_weights(), self.bias,

  Error (MYPY) [attr-defined]
    Module has no attribute "gru"

        579  |            result = _VF.gru(input, hx, self.get_flat_weights(), self.bias, self.num_layers,
        580  |                             self.dropout, self.training, self.bidirectional, self.batch_first)
        581  |        else:
    >>> 582  |            result = _VF.gru(input, batch_sizes, hx, self.get_flat_weights(), self.bias,
        583  |                             self.num_layers, self.dropout, self.training, self.bidirectional)
        584  |        output = result[0]
        585  |        hidden = result[1]



>>> Lint for torch/autograd/forward_ad.py:

  Error (MYPY) [attr-defined]
    Module has no attribute "_make_dual"

        124  |            f"Expected tangent to be floating point or complex, but got: {tangent.dtype}"
        125  |        )
        126  |
    >>> 127  |    return torch._VF._make_dual(tensor, tangent, level=level)
        128  |
        129  |
        130  |_UnpackedDualTensor = namedtuple("_UnpackedDualTensor", ["primal", "tangent"])

  Error (MYPY) [attr-defined]
    Module has no attribute "_unpack_dual"

        167  |    if level < 0:
        168  |        return UnpackedDualTensor(tensor, None)
        169  |
    >>> 170  |    primal, dual = torch._VF._unpack_dual(tensor, level=level)
        171  |
        172  |    return UnpackedDualTensor(primal, dual)
        173  |



>>> Lint for torch/autograd/profiler.py:

  Error (MYPY) [attr-defined]
    "_KinetoEvent" has no attribute "device_resource_id"

         511  |                sequence_nr=kineto_event.sequence_nr(),
         512  |                device_type=kineto_event.device_type(),
         513  |                device_index=kineto_event.device_index(),
    >>>  514  |                device_resource_id=kineto_event.device_resource_id(),
         515  |                flops=kineto_event.flops(),
         516  |            )
         517  |            max_evt_id = max(max_evt_id, fe.id)



>>> Lint for torch/backends/cuda/__init__.py:

  Error (MYPY) [attr-defined]
    Module has no attribute "_get_cudnn_sdp_enabled"; maybe
    "_get_math_sdp_enabled" or "_get_flash_sdp_enabled"?

        314  |
        315  |    Returns whether cuDNN scaled dot product attention is enabled or not.
        316  |    """
    >>> 317  |    return torch._C._get_cudnn_sdp_enabled()
        318  |
        319  |
        320  |def enable_cudnn_sdp(enabled: bool):

  Error (MYPY) [attr-defined]
    Module has no attribute "_set_sdp_use_cudnn"

        323  |
        324  |    Enables or disables cuDNN scaled dot product attention.
        325  |    """
    >>> 326  |    torch._C._set_sdp_use_cudnn(enabled)
        327  |
        328  |
        329  |@contextlib.contextmanager

  Error (MYPY) [attr-defined]
    "type[_SDPBackend]" has no attribute "CUDNN_ATTENTION"

        358  |    if enable_math:
        359  |        backend_list.append(SDPBackend.MATH)
        360  |    if enable_cudnn:
    >>> 361  |        backend_list.append(SDPBackend.CUDNN_ATTENTION)
        362  |
        363  |    with sdpa_kernel(backend_list) as context:
        364  |        try:



>>> Lint for torch/backends/mkl/__init__.py:

  Error (MYPY) [attr-defined]
    Module has no attribute "_verbose"

        45  |    def __enter__(self):
        46  |        if self.enable == VERBOSE_OFF:
        47  |            return
    >>> 48  |        st = torch._C._verbose.mkl_set_verbose(self.enable)
        49  |        assert (
        50  |            st
        51  |        ), "Failed to set MKL into verbose mode. Please consider to disable this verbose scope."

  Error (MYPY) [attr-defined]
    Module has no attribute "_verbose"

        52  |        return self
        53  |
        54  |    def __exit__(self, exc_type, exc_val, exc_tb):
    >>> 55  |        torch._C._verbose.mkl_set_verbose(VERBOSE_OFF)
        56  |        return False



>>> Lint for torch/backends/mkldnn/__init__.py:

  Error (MYPY) [attr-defined]
    Module has no attribute "_verbose"

        53  |    def __enter__(self):
        54  |        if self.level == VERBOSE_OFF:
        55  |            return
    >>> 56  |        st = torch._C._verbose.mkldnn_set_verbose(self.level)
        57  |        assert (
        58  |            st
        59  |        ), "Failed to set MKLDNN into verbose mode. Please consider to disable this verbose scope."

  Error (MYPY) [attr-defined]
    Module has no attribute "_verbose"

        60  |        return self
        61  |
        62  |    def __exit__(self, exc_type, exc_val, exc_tb):
    >>> 63  |        torch._C._verbose.mkldnn_set_verbose(VERBOSE_OFF)
        64  |        return False
        65  |
        66  |



>>> Lint for torch/backends/mps/__init__.py:

  Error (MYPY) [attr-defined]
    Module has no attribute "_mps_is_on_macos_or_newer"

        27  |@_lru_cache
        28  |def is_macos_or_newer(major: int, minor: int) -> bool:
        29  |    r"""Return a bool indicating whether MPS is running on given MacOS or newer."""
    >>> 30  |    return torch._C._mps_is_on_macos_or_newer(major, minor)
        31  |
        32  |
        33  |@_lru_cache

  Error (MYPY) [attr-defined]
    Module has no attribute "_mps_is_on_macos_or_newer"

        33  |@_lru_cache
        34  |def is_macos13_or_newer(minor: int = 0) -> bool:
        35  |    r"""Return a bool indicating whether MPS is running on MacOS 13 or newer."""
    >>> 36  |    return torch._C._mps_is_on_macos_or_newer(13, minor)
        37  |
        38  |
        39  |_lib: Optional[_Library] = None



>>> Lint for torch/backends/nnpack/__init__.py:

  Error (MYPY) [attr-defined]
    Module has no attribute "_get_nnpack_enabled"; maybe
    "_is_xnnpack_enabled", "_get_cudnn_enabled", or "_get_mkldnn_enabled"?

        13  |
        14  |def set_flags(_enabled):
        15  |    r"""Set if nnpack is enabled globally"""
    >>> 16  |    orig_flags = (torch._C._get_nnpack_enabled(),)
        17  |    torch._C._set_nnpack_enabled(_enabled)
        18  |    return orig_flags
        19  |

  Error (MYPY) [attr-defined]
    Module has no attribute "_set_nnpack_enabled"; maybe
    "_is_xnnpack_enabled", "_set_cudnn_enabled", or "_set_mkldnn_enabled"?

        14  |def set_flags(_enabled):
        15  |    r"""Set if nnpack is enabled globally"""
        16  |    orig_flags = (torch._C._get_nnpack_enabled(),)
    >>> 17  |    torch._C._set_nnpack_enabled(_enabled)
        18  |    return orig_flags
        19  |
        20  |



>>> Lint for torch/cuda/__init__.py:

  Error (MYPY) [attr-defined]
    Module has no attribute "_has_magma"

          88  |
          89  |
          90  |has_half: bool = True
    >>>   91  |has_magma: bool = torch._C._has_magma
          92  |
          93  |default_generators: Tuple[torch._C.Generator] = ()  # type: ignore[assignment]
          94  |



>>> Lint for torch/cuda/_sanitizer.py:

  Error (MYPY) [attr-defined]
    Module has no attribute "_activate_gpu_trace"; maybe
    "_activate_cuda_trace"?

        528  |class CUDASanitizerDispatchMode(TorchDispatchMode):
        529  |    def __init__(self):
        530  |        self.event_handler = EventHandler()
    >>> 531  |        torch._C._activate_gpu_trace()
        532  |        gpu_trace.register_callback_for_event_creation(
        533  |            self.event_handler._handle_event_creation
        534  |        )



>>> Lint for torch/distributed/_functional_collectives.py:

  Error (MYPY) [attr-defined]
    "ProcessGroup" has no attribute "group_name"

         725  |    # `tag` will be deprecated. See details in:
         726  |    # https://github.com/pytorch/pytorch/issues/93173#issuecomment-1907095208
         727  |    if isinstance(group, dist.ProcessGroup):
    >>>  728  |        return group.group_name
         729  |    elif isinstance(group, str):
         730  |        return group
         731  |    elif isinstance(group, DeviceMesh):



>>> Lint for torch/distributed/_shard/sharded_tensor/api.py:

  Error (MYPY) [attr-defined]
    "ProcessGroup" has no attribute "_get_backend_name"

         591  |        current_device: torch.device
         592  |        if self._local_shards:
         593  |            current_device = self._local_shards[0].tensor.device
    >>>  594  |        elif self._process_group._get_backend_name() == "gloo":
         595  |            current_device = torch.device("cpu")
         596  |        else:
         597  |            current_device = torch.device(torch.cuda.current_device())



>>> Lint for torch/distributed/_tensor/tp_conv.py:

  Error (MYPY) [arg-type]
    Argument 2 to "pad" has incompatible type "tuple[int, Any]"; expected
    "list[int]"

        194  |        padding_w = padding[1]
        195  |        if rank == 0:
        196  |            grad_out_tensor = torch.nn.functional.pad(
    >>> 197  |                grad_out_tensor, (0, padding_w), "constant", 0
        198  |            )
        199  |        elif rank == size - 1:
        200  |            grad_out_tensor = torch.nn.functional.pad(

  Error (MYPY) [arg-type]
    Argument 2 to "pad" has incompatible type "tuple[Any, int]"; expected
    "list[int]"

        198  |            )
        199  |        elif rank == size - 1:
        200  |            grad_out_tensor = torch.nn.functional.pad(
    >>> 201  |                grad_out_tensor, (padding_w, 0), "constant", 0
        202  |            )
        203  |        else:
        204  |            grad_out_tensor = torch.nn.functional.pad(

  Error (MYPY) [arg-type]
    Argument 2 to "pad" has incompatible type "tuple[Any, Any]"; expected
    "list[int]"

        202  |            )
        203  |        else:
        204  |            grad_out_tensor = torch.nn.functional.pad(
    >>> 205  |                grad_out_tensor, (padding_w, padding_w), "constant", 0
        206  |            )
        207  |
        208  |        # step3 feed local input tensor to op_call



>>> Lint for torch/distributed/algorithms/ddp_comm_hooks/quantization_hooks.py:

  Error (MYPY) [arg-type]
    Argument "pad" to "pad" has incompatible type "tuple[int, Any]"; expected
    "list[int]"

        153  |    tensor_in_channels = (
        154  |        nn.functional.pad(
        155  |            input=tensor,
    >>> 156  |            pad=(0, bucket_size - len(tensor) % bucket_size),
        157  |            mode="constant",
        158  |            value=0,
        159  |        )



>>> Lint for torch/distributed/device_mesh.py:

  Error (MYPY) [attr-defined]
    "ProcessGroup" has no attribute "group_name"

        283  |                    (
        284  |                        _get_group_tag(_get_default_group()),
        285  |                        list(range(get_world_size())),
    >>> 286  |                        _get_default_group().group_name,
        287  |                    )
        288  |                )
        289  |            else:



>>> Lint for torch/distributed/distributed_c10d.py:

  Error (MYPY) [attr-defined]
    Module "torch._C._distributed_c10d" has no attribute
    "_DistributedBackendOptions"

          16  |from typing import Any, Callable, Dict, Optional, Tuple, Union, List, TYPE_CHECKING
          17  |
          18  |import torch
    >>>   19  |from torch._C._distributed_c10d import (
          20  |    AllgatherOptions,
          21  |    AllreduceCoalescedOptions,
          22  |    AllreduceOptions,

  Error (MYPY) [attr-defined]
    Module "torch._C._distributed_c10d" has no attribute
    "_register_process_group"

          16  |from typing import Any, Callable, Dict, Optional, Tuple, Union, List, TYPE_CHECKING
          17  |
          18  |import torch
    >>>   19  |from torch._C._distributed_c10d import (
          20  |    AllgatherOptions,
          21  |    AllreduceCoalescedOptions,
          22  |    AllreduceOptions,

  Error (MYPY) [attr-defined]
    Module "torch._C._distributed_c10d" has no attribute
    "_resolve_process_group"

          16  |from typing import Any, Callable, Dict, Optional, Tuple, Union, List, TYPE_CHECKING
          17  |
          18  |import torch
    >>>   19  |from torch._C._distributed_c10d import (
          20  |    AllgatherOptions,
          21  |    AllreduceCoalescedOptions,
          22  |    AllreduceOptions,

  Error (MYPY) [attr-defined]
    Module "torch._C._distributed_c10d" has no attribute
    "_unregister_all_process_groups"

          16  |from typing import Any, Callable, Dict, Optional, Tuple, Union, List, TYPE_CHECKING
          17  |
          18  |import torch
    >>>   19  |from torch._C._distributed_c10d import (
          20  |    AllgatherOptions,
          21  |    AllreduceCoalescedOptions,
          22  |    AllreduceOptions,

  Error (MYPY) [attr-defined]
    Module "torch._C._distributed_c10d" has no attribute
    "_unregister_process_group"

          16  |from typing import Any, Callable, Dict, Optional, Tuple, Union, List, TYPE_CHECKING
          17  |
          18  |import torch
    >>>   19  |from torch._C._distributed_c10d import (
          20  |    AllgatherOptions,
          21  |    AllreduceCoalescedOptions,
          22  |    AllreduceOptions,

  Error (MYPY) [name-defined]
    Name "ProcessGroup.BackendType" is not defined

         198  |        MPI : ["cpu", "cuda"],
         199  |    }
         200  |
    >>>  201  |    backend_type_map: Dict[str, ProcessGroup.BackendType] = {
         202  |        UNDEFINED: ProcessGroup.BackendType.UNDEFINED,
         203  |        GLOO : ProcessGroup.BackendType.GLOO,
         204  |        NCCL: ProcessGroup.BackendType.NCCL,

  Error (MYPY) [attr-defined]
    "type[ProcessGroup]" has no attribute "BackendType"

         199  |    }
         200  |
         201  |    backend_type_map: Dict[str, ProcessGroup.BackendType] = {
    >>>  202  |        UNDEFINED: ProcessGroup.BackendType.UNDEFINED,
         203  |        GLOO : ProcessGroup.BackendType.GLOO,
         204  |        NCCL: ProcessGroup.BackendType.NCCL,
         205  |        UCC: ProcessGroup.BackendType.UCC,

  Error (MYPY) [attr-defined]
    "type[ProcessGroup]" has no attribute "BackendType"

         200  |
         201  |    backend_type_map: Dict[str, ProcessGroup.BackendType] = {
         202  |        UNDEFINED: ProcessGroup.BackendType.UNDEFINED,
    >>>  203  |        GLOO : ProcessGroup.BackendType.GLOO,
         204  |        NCCL: ProcessGroup.BackendType.NCCL,
         205  |        UCC: ProcessGroup.BackendType.UCC,
         206  |    }

  Error (MYPY) [attr-defined]
    "type[ProcessGroup]" has no attribute "BackendType"

         201  |    backend_type_map: Dict[str, ProcessGroup.BackendType] = {
         202  |        UNDEFINED: ProcessGroup.BackendType.UNDEFINED,
         203  |        GLOO : ProcessGroup.BackendType.GLOO,
    >>>  204  |        NCCL: ProcessGroup.BackendType.NCCL,
         205  |        UCC: ProcessGroup.BackendType.UCC,
         206  |    }
         207  |

  Error (MYPY) [attr-defined]
    "type[ProcessGroup]" has no attribute "BackendType"

         202  |        UNDEFINED: ProcessGroup.BackendType.UNDEFINED,
         203  |        GLOO : ProcessGroup.BackendType.GLOO,
         204  |        NCCL: ProcessGroup.BackendType.NCCL,
    >>>  205  |        UCC: ProcessGroup.BackendType.UCC,
         206  |    }
         207  |
         208  |    def __new__(cls, name: str):

  Error (MYPY) [attr-defined]
    "type[ProcessGroup]" has no attribute "BackendType"

         257  |            for device in devices:
         258  |                if device != 'cpu' and device != 'cuda':
         259  |                    Backend.default_device_backend_map[device] = name.lower()
    >>>  260  |        Backend.backend_type_map[name.lower()] = ProcessGroup.BackendType.CUSTOM
         261  |
         262  |        # Update device capability matrix in Backend class
         263  |        if devices is None:

  Error (MYPY) [attr-defined]
    "ProcessGroup" has no attribute "_device_types"

         669  |    ("cpu", "cuda", etc) supported by ``group``. Can be multiple if the
         670  |    ``group`` supports multiple devices.
         671  |    """
    >>>  672  |    devices = group._device_types
         673  |
         674  |    if len(devices) == 1:
         675  |        # User fixed exactly one backend in `init_process_group`

  Error (MYPY) [attr-defined]
    "ProcessGroup" has no attribute "group_name"

         854  |    group = _find_pg_by_ranks_and_tag(tag, ranks)
         855  |    if group is None:
         856  |        raise ValueError("")
    >>>  857  |    return group.group_name
         858  |
         859  |
         860  |def _check_single_tensor(param, param_name) -> None:

  Error (MYPY) [attr-defined]
    Module has no attribute "_set_global_rank"

        1030  |def _update_default_pg(pg) -> None:
        1031  |    _world.default_pg = pg
        1032  |    rank = pg.rank() if pg is not None and pg != GroupMember.NON_GROUP_MEMBER else -1
    >>> 1033  |    torch._C._distributed_c10d._set_global_rank(rank)
        1034  |
        1035  |def get_backend_config(group: Optional[ProcessGroup] = None) -> str:
        1036  |    """

  Error (MYPY) [attr-defined]
    "ProcessGroup" has no attribute "_get_backend"

        1079  |def _get_process_group_uid(pg: ProcessGroup) -> int:
        1080  |    backend = None
        1081  |    try:
    >>> 1082  |        backend = pg._get_backend(torch.device("cuda"))
        1083  |    except RuntimeError:
        1084  |        pass
        1085  |    if is_nccl_available() and isinstance(backend, ProcessGroupNCCL):

  Error (MYPY) [attr-defined]
    "ProcessGroupNCCL" has no attribute "uid"

        1083  |    except RuntimeError:
        1084  |        pass
        1085  |    if is_nccl_available() and isinstance(backend, ProcessGroupNCCL):
    >>> 1086  |        return backend.uid
        1087  |    return -1
        1088  |
        1089  |def _get_pg_config(group: Optional[ProcessGroup] = None) -> Dict[str, Any]:

  Error (MYPY) [attr-defined]
    "ProcessGroup" has no attribute "_device_types"

        1145  |    if _rank_not_in_group(group):
        1146  |        raise ValueError("Invalid process group specified")
        1147  |    assert isinstance(group, ProcessGroup)
    >>> 1148  |    devices = group._device_types
        1149  |    backends = set()
        1150  |    if torch.device("cpu") in devices and is_gloo_available():
        1151  |        backend = group._get_backend(torch.device("cpu"))

  Error (MYPY) [attr-defined]
    "ProcessGroup" has no attribute "_get_backend"

        1148  |    devices = group._device_types
        1149  |    backends = set()
        1150  |    if torch.device("cpu") in devices and is_gloo_available():
    >>> 1151  |        backend = group._get_backend(torch.device("cpu"))
        1152  |        if isinstance(backend, ProcessGroupGloo):
        1153  |            backends.add(backend)
        1154  |    if torch.device("cuda") in devices:

  Error (MYPY) [attr-defined]
    "ProcessGroup" has no attribute "_get_backend"

        1152  |        if isinstance(backend, ProcessGroupGloo):
        1153  |            backends.add(backend)
        1154  |    if torch.device("cuda") in devices:
    >>> 1155  |        backend = group._get_backend(torch.device("cuda"))
        1156  |        if is_nccl_available() and isinstance(backend, ProcessGroupNCCL):
        1157  |            backends.add(backend)  # type: ignore[arg-type]
        1158  |        elif is_gloo_available() and isinstance(backend, ProcessGroupGloo):

  Error (MYPY) [attr-defined]
    "ProcessGroupNCCL" has no attribute "_shutdown"

        1401  |        pass
        1402  |    if isinstance(backend, ProcessGroupNCCL):
        1403  |        # explictly call shutdown to ensure that NCCL resources are released
    >>> 1404  |        backend._shutdown()
        1405  |
        1406  |def _new_process_group_helper(
        1407  |    group_size,

  Error (MYPY) [attr-defined]
    "ProcessGroup" has no attribute "bound_device_id"

        1458  |    # entire world or if we have bound a device id to the world (which
        1459  |    # causes early connection initialization).
        1460  |    if (is_initialized() and
    >>> 1461  |            (len(global_ranks_in_group) == _get_default_group().size() or _get_default_group().bound_device_id)):
        1462  |        split_from = _get_split_source(_get_default_group())
        1463  |    else:
        1464  |        split_from = None

  Error (MYPY) [attr-defined]
    "ProcessGroup" has no attribute "bound_device_id"

        1475  |            # a requirement of the NCCL API as otherwise we would get
        1476  |            # out of sync.
        1477  |            if split_from:
    >>> 1478  |                split_from.perform_nocolor_split(_get_default_group().bound_device_id)
        1479  |            return GroupMember.NON_GROUP_MEMBER, None
        1480  |
        1481  |    prefix_store = PrefixStore(f"{group_name}/", store)

  Error (MYPY) [call-arg]
    Unexpected keyword argument "backend" for "Options"

        1479  |            return GroupMember.NON_GROUP_MEMBER, None
        1480  |
        1481  |    prefix_store = PrefixStore(f"{group_name}/", store)
    >>> 1482  |    base_pg_options = ProcessGroup.Options(backend=str(backend))
        1483  |    base_pg_options._timeout = timeout
        1484  |    pg: ProcessGroup = ProcessGroup(prefix_store, group_rank, group_size, base_pg_options)
        1485  |    if device_id:

  Error (MYPY) [attr-defined]
    "Options" has no attribute "_timeout"

        1480  |
        1481  |    prefix_store = PrefixStore(f"{group_name}/", store)
        1482  |    base_pg_options = ProcessGroup.Options(backend=str(backend))
    >>> 1483  |    base_pg_options._timeout = timeout
        1484  |    pg: ProcessGroup = ProcessGroup(prefix_store, group_rank, group_size, base_pg_options)
        1485  |    if device_id:
        1486  |        pg.bound_device_id = device_id

  Error (MYPY) [call-arg]
    Too many arguments for "ProcessGroup"

        1481  |    prefix_store = PrefixStore(f"{group_name}/", store)
        1482  |    base_pg_options = ProcessGroup.Options(backend=str(backend))
        1483  |    base_pg_options._timeout = timeout
    >>> 1484  |    pg: ProcessGroup = ProcessGroup(prefix_store, group_rank, group_size, base_pg_options)
        1485  |    if device_id:
        1486  |        pg.bound_device_id = device_id
        1487  |    backend_config = BackendConfig(backend)

  Error (MYPY) [attr-defined]
    "ProcessGroup" has no attribute "bound_device_id"

        1483  |    base_pg_options._timeout = timeout
        1484  |    pg: ProcessGroup = ProcessGroup(prefix_store, group_rank, group_size, base_pg_options)
        1485  |    if device_id:
    >>> 1486  |        pg.bound_device_id = device_id
        1487  |    backend_config = BackendConfig(backend)
        1488  |    backend_class: torch._C._distributed_c10d.Backend
        1489  |    for device, backend_str in backend_config.get_device_backend_map().items():

  Error (MYPY) [assignment]
    Incompatible types in assignment (expression has type "ProcessGroupMPI",
    variable has type "Backend")

        1498  |                    " MPI is only included if you build PyTorch from"
        1499  |                    " source on a host that has MPI installed."
        1500  |                )
    >>> 1501  |            backend_class = ProcessGroupMPI.create(global_ranks_in_group)
        1502  |            backend_type = ProcessGroup.BackendType.MPI
        1503  |            if not backend_class:
        1504  |                return GroupMember.NON_GROUP_MEMBER, None

  Error (MYPY) [attr-defined]
    "type[ProcessGroup]" has no attribute "BackendType"

        1499  |                    " source on a host that has MPI installed."
        1500  |                )
        1501  |            backend_class = ProcessGroupMPI.create(global_ranks_in_group)
    >>> 1502  |            backend_type = ProcessGroup.BackendType.MPI
        1503  |            if not backend_class:
        1504  |                return GroupMember.NON_GROUP_MEMBER, None
        1505  |            # create new process group with accurate rank and size

  Error (MYPY) [call-arg]
    Too many arguments for "ProcessGroup"

        1504  |                return GroupMember.NON_GROUP_MEMBER, None
        1505  |            # create new process group with accurate rank and size
        1506  |            if pg.rank() == -1 and pg.size() == -1:
    >>> 1507  |                pg = ProcessGroup(backend_prefix_store, backend_class.rank(), backend_class.size(), base_pg_options)
        1508  |        elif backend_str == Backend.GLOO:
        1509  |            # TODO: remove this check after lazy initialization is supported
        1510  |            # if pg_options is not None:

  Error (MYPY) [attr-defined]
    "Backend" has no attribute "rank"

        1504  |                return GroupMember.NON_GROUP_MEMBER, None
        1505  |            # create new process group with accurate rank and size
        1506  |            if pg.rank() == -1 and pg.size() == -1:
    >>> 1507  |                pg = ProcessGroup(backend_prefix_store, backend_class.rank(), backend_class.size(), base_pg_options)
        1508  |        elif backend_str == Backend.GLOO:
        1509  |            # TODO: remove this check after lazy initialization is supported
        1510  |            # if pg_options is not None:

  Error (MYPY) [attr-defined]
    "Backend" has no attribute "size"

        1504  |                return GroupMember.NON_GROUP_MEMBER, None
        1505  |            # create new process group with accurate rank and size
        1506  |            if pg.rank() == -1 and pg.size() == -1:
    >>> 1507  |                pg = ProcessGroup(backend_prefix_store, backend_class.rank(), backend_class.size(), base_pg_options)
        1508  |        elif backend_str == Backend.GLOO:
        1509  |            # TODO: remove this check after lazy initialization is supported
        1510  |            # if pg_options is not None:

  Error (MYPY) [assignment]
    Incompatible types in assignment (expression has type "ProcessGroupGloo",
    variable has type "Backend")

        1509  |            # TODO: remove this check after lazy initialization is supported
        1510  |            # if pg_options is not None:
        1511  |            #     raise RuntimeError("GLOO options not supported")
    >>> 1512  |            backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
        1513  |            backend_type = ProcessGroup.BackendType.GLOO
        1514  |        elif backend_str == Backend.NCCL:
        1515  |            if not is_nccl_available():

  Error (MYPY) [attr-defined]
    "type[ProcessGroup]" has no attribute "BackendType"

        1510  |            # if pg_options is not None:
        1511  |            #     raise RuntimeError("GLOO options not supported")
        1512  |            backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
    >>> 1513  |            backend_type = ProcessGroup.BackendType.GLOO
        1514  |        elif backend_str == Backend.NCCL:
        1515  |            if not is_nccl_available():
        1516  |                raise RuntimeError("Distributed package doesn't have NCCL built in")

  Error (MYPY) [attr-defined]
    "Options" has no attribute "_timeout"

        1518  |                assert isinstance(
        1519  |                    pg_options, ProcessGroupNCCL.Options
        1520  |                ), "Expected pg_options argument to be of type ProcessGroupNCCL.Options"
    >>> 1521  |                if pg_options._timeout != timeout:
        1522  |                    warnings.warn(
        1523  |                        "pg_options._timeout was specified, "
        1524  |                        "but timeout kwarg has a default value that will always override it. "

  Error (MYPY) [assignment]
    Incompatible types in assignment (expression has type "ProcessGroupNCCL",
    variable has type "Backend")

        1534  |                pg_options.split_color = _process_group_color(global_ranks_in_group)
        1535  |            pg_options.global_ranks_in_group = global_ranks_in_group
        1536  |            pg_options.group_name = group_name
    >>> 1537  |            backend_class = ProcessGroupNCCL(
        1538  |                backend_prefix_store, group_rank, group_size, pg_options)
        1539  |            backend_type = ProcessGroup.BackendType.NCCL
        1540  |        elif backend_str == Backend.UCC and is_ucc_available():

  Error (MYPY) [attr-defined]
    "type[ProcessGroup]" has no attribute "BackendType"

        1536  |            pg_options.group_name = group_name
        1537  |            backend_class = ProcessGroupNCCL(
        1538  |                backend_prefix_store, group_rank, group_size, pg_options)
    >>> 1539  |            backend_type = ProcessGroup.BackendType.NCCL
        1540  |        elif backend_str == Backend.UCC and is_ucc_available():
        1541  |            # TODO: once UCC plugin is fully deprecated, remove
        1542  |            # is_ucc_available() from above elif-condition and raise

  Error (MYPY) [assignment]
    Incompatible types in assignment (expression has type "ProcessGroupUCC",
    variable has type "Backend")

        1542  |            # is_ucc_available() from above elif-condition and raise
        1543  |            # RuntimeError if is_ucc_available() returns false.
        1544  |
    >>> 1545  |            backend_class = ProcessGroupUCC(backend_prefix_store, group_rank, group_size, timeout=timeout)
        1546  |            backend_type = ProcessGroup.BackendType.UCC
        1547  |        else:
        1548  |            assert backend_str.upper() in Backend._plugins, (

  Error (MYPY) [attr-defined]
    "type[ProcessGroup]" has no attribute "BackendType"

        1543  |            # RuntimeError if is_ucc_available() returns false.
        1544  |
        1545  |            backend_class = ProcessGroupUCC(backend_prefix_store, group_rank, group_size, timeout=timeout)
    >>> 1546  |            backend_type = ProcessGroup.BackendType.UCC
        1547  |        else:
        1548  |            assert backend_str.upper() in Backend._plugins, (
        1549  |                f"Unknown c10d backend type {backend_str.upper()}"

  Error (MYPY) [attr-defined]
    "type[ProcessGroup]" has no attribute "BackendType"

        1552  |            backend_plugin = Backend._plugins[backend_str.upper()]
        1553  |            creator_fn = backend_plugin.creator_fn
        1554  |            extended_api = backend_plugin.extended_api
    >>> 1555  |            backend_type = ProcessGroup.BackendType.CUSTOM
        1556  |
        1557  |            if not extended_api:
        1558  |                backend_class = creator_fn(backend_prefix_store, group_rank, group_size, timeout)

  Error (MYPY) [attr-defined]
    "<subclass of "Backend" and "ProcessGroupGloo">" has no attribute
    "_set_sequence_number_for_group"

        1570  |        # Set sequence numbers for gloo and nccl backends.
        1571  |        if backend_str == Backend.GLOO:
        1572  |            assert isinstance(backend_class, ProcessGroupGloo)
    >>> 1573  |            backend_class._set_sequence_number_for_group()
        1574  |        elif backend_str == Backend.NCCL:
        1575  |            assert isinstance(backend_class, ProcessGroupNCCL)
        1576  |            backend_class._set_sequence_number_for_group()

  Error (MYPY) [attr-defined]
    "<subclass of "Backend" and "ProcessGroupNCCL">" has no attribute
    "_set_sequence_number_for_group"

        1573  |            backend_class._set_sequence_number_for_group()
        1574  |        elif backend_str == Backend.NCCL:
        1575  |            assert isinstance(backend_class, ProcessGroupNCCL)
    >>> 1576  |            backend_class._set_sequence_number_for_group()
        1577  |
        1578  |        # If the type is a subclass of ProcessGroup then return this process group immediately
        1579  |        # TODO: This defaults to the old behavior for PythonProcessGroups which overwrites the

  Error (MYPY) [attr-defined]
    "ProcessGroup" has no attribute "_register_backend"

        1607  |        # register only a single backend when all get_device_backend_map values are the same
        1608  |        if len(set(backend_config.get_device_backend_map().values())) == 1:
        1609  |            for device in backend_config.get_device_backend_map().keys():
    >>> 1610  |                pg._register_backend(torch.device(device), backend_type, backend_class)
        1611  |
        1612  |            # break out of outer loop to not create any more backends
        1613  |            break

  Error (MYPY) [attr-defined]
    "ProcessGroup" has no attribute "_register_backend"

        1612  |            # break out of outer loop to not create any more backends
        1613  |            break
        1614  |
    >>> 1615  |        pg._register_backend(torch.device(device), backend_type, backend_class)
        1616  |
        1617  |    if device_id and pg._get_backend(device_id).supports_splitting:
        1618  |        eager_backend = pg._get_backend(device_id)

  Error (MYPY) [attr-defined]
    "ProcessGroup" has no attribute "_get_backend"

        1614  |
        1615  |        pg._register_backend(torch.device(device), backend_type, backend_class)
        1616  |
    >>> 1617  |    if device_id and pg._get_backend(device_id).supports_splitting:
        1618  |        eager_backend = pg._get_backend(device_id)
        1619  |        eager_backend.eager_connect_single_device(device_id)
        1620  |

  Error (MYPY) [attr-defined]
    "ProcessGroup" has no attribute "_get_backend"

        1615  |        pg._register_backend(torch.device(device), backend_type, backend_class)
        1616  |
        1617  |    if device_id and pg._get_backend(device_id).supports_splitting:
    >>> 1618  |        eager_backend = pg._get_backend(device_id)
        1619  |        eager_backend.eager_connect_single_device(device_id)
        1620  |
        1621  |    # update global state

  Error (MYPY) [attr-defined]
    "ProcessGroup" has no attribute "_set_group_name"

        1622  |    assert group_name is not None
        1623  |    _world.pg_map[pg] = (backend, prefix_store)
        1624  |    _world.pg_names[pg] = group_name
    >>> 1625  |    pg._set_group_name(group_name)
        1626  |    _register_process_group(group_name, pg)
        1627  |
        1628  |    _world.pg_backend_config[pg] = str(backend_config)

  Error (MYPY) [attr-defined]
    "ProcessGroup" has no attribute "name"

        1669  |    # alive until all works and hooks are done. The current implementation does the
        1670  |    # latter. Therefore, we explicitly call _wait_for_pending_works() here to wait
        1671  |    # for the pending hooks to finish.
    >>> 1672  |    if pg.name().lower() == "nccl" and pg._has_hooks():
        1673  |        pg._wait_for_pending_works()
        1674  |
        1675  |    if group is None or group == GroupMember.WORLD:

  Error (MYPY) [attr-defined]
    "ProcessGroup" has no attribute "_has_hooks"

        1669  |    # alive until all works and hooks are done. The current implementation does the
        1670  |    # latter. Therefore, we explicitly call _wait_for_pending_works() here to wait
        1671  |    # for the pending hooks to finish.
    >>> 1672  |    if pg.name().lower() == "nccl" and pg._has_hooks():
        1673  |        pg._wait_for_pending_works()
        1674  |
        1675  |    if group is None or group == GroupMember.WORLD:

  Error (MYPY) [attr-defined]
    "ProcessGroup" has no attribute "_wait_for_pending_works"

        1670  |    # latter. Therefore, we explicitly call _wait_for_pending_works() here to wait
        1671  |    # for the pending hooks to finish.
        1672  |    if pg.name().lower() == "nccl" and pg._has_hooks():
    >>> 1673  |        pg._wait_for_pending_works()
        1674  |
        1675  |    if group is None or group == GroupMember.WORLD:
        1676  |        if _abort_in_destroy_pg():

  Error (MYPY) [attr-defined]
    "ProcessGroup" has no attribute "group_name"

        1724  |                    _world.tags_to_pg[""].remove(pg)
        1725  |            except Exception:
        1726  |                pass
    >>> 1727  |        _unregister_process_group(pg.group_name)
        1728  |
        1729  |
        1730  |def get_rank(group: Optional[ProcessGroup] = None) -> int:

  Error (MYPY) [attr-defined]
    "ProcessGroup" has no attribute "_start_coalescing"

        1994  |    if op_list:
        1995  |        raise ValueError("ProcessGroup has non-empty op list at the start of coalescing")
        1996  |    if device:
    >>> 1997  |        group._start_coalescing(device)
        1998  |    cm = _CoalescingManager()
        1999  |    yield cm
        2000  |    op_list = _world.pg_coalesce_state.pop(group)

  Error (MYPY) [attr-defined]
    "ProcessGroup" has no attribute "allgather_into_tensor_coalesced"; maybe
    "allgather_coalesced"?

        2019  |            for op in op_list:
        2020  |                inputs.append(op.tensor)
        2021  |                outputs.append(not_none(op.dst_tensor))
    >>> 2022  |            work = group.allgather_into_tensor_coalesced(outputs, inputs)
        2023  |        elif op0 == reduce_scatter_tensor:
        2024  |            inputs = []
        2025  |            outputs = []

  Error (MYPY) [attr-defined]
    "ProcessGroup" has no attribute "reduce_scatter_tensor_coalesced"

        2028  |                outputs.append(not_none(op.dst_tensor))
        2029  |            reduce_opts = ReduceScatterOptions()
        2030  |            reduce_opts.reduceOp = not_none(op_list[0].redop)
    >>> 2031  |            work = group.reduce_scatter_tensor_coalesced(outputs, inputs, reduce_opts)
        2032  |        else:
        2033  |            raise AssertionError(
        2034  |                f"Coalescing manager does not support fast-path coalescing of {op0}, "

  Error (MYPY) [attr-defined]
    "ProcessGroup" has no attribute "_end_coalescing"

        2037  |
        2038  |    if device:
        2039  |        # Old style of letting each coll inside the context manager to call into C++ counterpart via python binding
    >>> 2040  |        work = group._end_coalescing(device)
        2041  |
        2042  |    if async_ops:
        2043  |        cm.append(work)  # type: ignore[possibly-undefined]

  Error (MYPY) [attr-defined]
    "Work" has no attribute "get_future"

        2289  |        work = group.allreduce_coalesced(tensors, opts)
        2290  |
        2291  |    if async_op:
    >>> 2292  |        return work.get_future()
        2293  |    else:
        2294  |        work.wait()
        2295  |

  Error (MYPY) [attr-defined]
    Module has no attribute "_hash_tensors"

        2349  |    if get_debug_level() == DebugLevel.DETAIL and is_nccl_available():
        2350  |        backend = get_backend(group)
        2351  |        if backend == Backend.NCCL:
    >>> 2352  |            hash = torch._C._distributed_c10d._hash_tensors([byte_tensor])
        2353  |            logger.warning("_object_to_tensor size: %s hash value: %s", byte_tensor.numel(), hash)
        2354  |    local_size = torch.LongTensor([byte_tensor.numel()]).to(device)
        2355  |    return byte_tensor, local_size

  Error (MYPY) [attr-defined]
    Module has no attribute "_hash_tensors"

        2359  |    if get_debug_level() == DebugLevel.DETAIL and is_nccl_available():
        2360  |        backend = get_backend(group)
        2361  |        if backend == Backend.NCCL:
    >>> 2362  |            hash = torch._C._distributed_c10d._hash_tensors([tensor])
        2363  |            logger.warning("_tensor_to_object size: %s hash value: %s", tensor.numel(), hash)
        2364  |    tensor = tensor.cpu()
        2365  |    buf = tensor.numpy().tobytes()[:tensor_size]

  Error (MYPY) [attr-defined]
    "Work" has no attribute "get_future"

        3069  |        work = group.allgather_coalesced(output_tensor_lists, input_tensor_list)
        3070  |
        3071  |    if async_op:
    >>> 3072  |        return work.get_future()
        3073  |    else:
        3074  |        work.wait()
        3075  |

  Error (MYPY) [call-arg]
    Too many arguments for "_reduce_scatter_base" of "ProcessGroup"

        3360  |        else:
        3361  |            return None
        3362  |
    >>> 3363  |    work = group._reduce_scatter_base(output, input, opts)
        3364  |
        3365  |    if async_op:
        3366  |        return work

  Error (MYPY) [attr-defined]
    "BarrierOptions" has no attribute "device"

        3673  |        return
        3674  |
        3675  |    opts = BarrierOptions()
    >>> 3676  |    opts.device = _get_pg_default_device(group)
        3677  |    if device_ids is not None:
        3678  |        if isinstance(device_ids, list):
        3679  |            opts.device_ids = device_ids

  Error (MYPY) [arg-type]
    Argument 1 to "_ProcessGroupWrapper" has incompatible type "Backend";
    expected "ProcessGroup"

        3782  |    store = PrefixStore(prefix, store)
        3783  |    helper_pg = ProcessGroupGloo(store, rank, world_size, timeout=timeout)
        3784  |    # Wrap the underlying pg with ProcessGroupWrapper.
    >>> 3785  |    wrapped_pg = _ProcessGroupWrapper(wrapped_pg, helper_pg)
        3786  |    return wrapped_pg
        3787  |
        3788  |# helper function for deterministically hashing a list of ranks



>>> Lint for torch/functional.py:

  Error (MYPY) [attr-defined]
    Module has no attribute "unique_dim"

         899  |            return_counts=return_counts, dim=dim)
         900  |
         901  |    if dim is not None:
    >>>  902  |        output, inverse_indices, counts = _VF.unique_dim(
         903  |            input,
         904  |            dim,
         905  |            sorted=sorted,

  Error (MYPY) [attr-defined]
    Module has no attribute "frobenius_norm"

        1638  |    if dim is None and out is None and dtype is None and p is not None:
        1639  |        if isinstance(p, str):
        1640  |            if p == "fro":
    >>> 1641  |                return _VF.frobenius_norm(input, dim=(), keepdim=keepdim)
        1642  |        if not isinstance(p, str):
        1643  |            _dim = [i for i in range(ndim)]  # noqa: C416 TODO: rewrite as list(range(m))
        1644  |            return _VF.norm(input, p, dim=_dim, keepdim=keepdim)  # type: ignore[attr-defined]

  Error (MYPY) [attr-defined]
    Module has no attribute "frobenius_norm"

        1662  |            if _dim is None:
        1663  |                _dim = list(range(ndim))
        1664  |            if out is None:
    >>> 1665  |                return _VF.frobenius_norm(input, _dim, keepdim=keepdim)  # type: ignore[arg-type]
        1666  |            else:
        1667  |                return _VF.frobenius_norm(input, _dim, keepdim=keepdim, out=out)  # type: ignore[arg-type]
        1668  |        elif p == "nuc":

  Error (MYPY) [attr-defined]
    Module has no attribute "frobenius_norm"

        1664  |            if out is None:
        1665  |                return _VF.frobenius_norm(input, _dim, keepdim=keepdim)  # type: ignore[arg-type]
        1666  |            else:
    >>> 1667  |                return _VF.frobenius_norm(input, _dim, keepdim=keepdim, out=out)  # type: ignore[arg-type]
        1668  |        elif p == "nuc":
        1669  |            if dtype is not None:
        1670  |                raise ValueError("dtype argument is not supported in nuclear norm")

  Error (MYPY) [attr-defined]
    Module has no attribute "nuclear_norm"

        1670  |                raise ValueError("dtype argument is not supported in nuclear norm")
        1671  |            if _dim is None:
        1672  |                if out is None:
    >>> 1673  |                    return _VF.nuclear_norm(input, keepdim=keepdim)  # type: ignore[arg-type]
        1674  |                else:
        1675  |                    return _VF.nuclear_norm(input, keepdim=keepdim, out=out)  # type: ignore[arg-type]
        1676  |            else:

  Error (MYPY) [attr-defined]
    Module has no attribute "nuclear_norm"

        1672  |                if out is None:
        1673  |                    return _VF.nuclear_norm(input, keepdim=keepdim)  # type: ignore[arg-type]
        1674  |                else:
    >>> 1675  |                    return _VF.nuclear_norm(input, keepdim=keepdim, out=out)  # type: ignore[arg-type]
        1676  |            else:
        1677  |                if out is None:
        1678  |                    return _VF.nuclear_norm(input, _dim, keepdim=keepdim)  # type: ignore[arg-type]

  Error (MYPY) [attr-defined]
    Module has no attribute "nuclear_norm"

        1675  |                    return _VF.nuclear_norm(input, keepdim=keepdim, out=out)  # type: ignore[arg-type]
        1676  |            else:
        1677  |                if out is None:
    >>> 1678  |                    return _VF.nuclear_norm(input, _dim, keepdim=keepdim)  # type: ignore[arg-type]
        1679  |                else:
        1680  |                    return _VF.nuclear_norm(input, _dim, keepdim=keepdim, out=out)  # type: ignore[arg-type]
        1681  |        raise RuntimeError(f"only valid string values are 'fro' and 'nuc', found {p}")

  Error (MYPY) [attr-defined]
    Module has no attribute "nuclear_norm"

        1677  |                if out is None:
        1678  |                    return _VF.nuclear_norm(input, _dim, keepdim=keepdim)  # type: ignore[arg-type]
        1679  |                else:
    >>> 1680  |                    return _VF.nuclear_norm(input, _dim, keepdim=keepdim, out=out)  # type: ignore[arg-type]
        1681  |        raise RuntimeError(f"only valid string values are 'fro' and 'nuc', found {p}")
        1682  |    else:
        1683  |        if _dim is None:

  Error (MYPY) [return-value]
    Incompatible return value type (got "list[Tensor]", expected
    "tuple[Tensor, ...]")

        1747  |        return handle_torch_function(
        1748  |            unravel_index, (indices,), indices, shape=shape)
        1749  |    res_tensor = _unravel_index(indices, shape)
    >>> 1750  |    return res_tensor.unbind(-1)
        1751  |
        1752  |def _unravel_index(indices: Tensor, shape: Union[int, Sequence[int]]) -> Tensor:
        1753  |    torch._check_type(



>>> Lint for torch/fx/graph.py:

  Error (MYPY) [attr-defined]
    Module has no attribute "uint32"

         220  |    torch.int64: 'i64',
         221  |    torch.bool: 'b8',
         222  |    torch.uint8: 'u8',
    >>>  223  |    torch.uint32: 'u32',
         224  |    torch.uint64: 'u64',
         225  |}
         226  |

  Error (MYPY) [attr-defined]
    Module has no attribute "uint64"

         221  |    torch.bool: 'b8',
         222  |    torch.uint8: 'u8',
         223  |    torch.uint32: 'u32',
    >>>  224  |    torch.uint64: 'u64',
         225  |}
         226  |
         227  |@compatibility(is_backward_compatible=True)



>>> Lint for torch/jit/_builtins.py:

  Error (MYPY) [attr-defined]
    Module has no attribute "unique_dim"

        101  |    (torch._VF.istft, "aten::istft"),  # type: ignore[attr-defined]
        102  |    (torch._VF.cdist, "aten::cdist"),  # type: ignore[attr-defined]
        103  |    (torch._VF.norm, "aten::norm"),  # type: ignore[attr-defined]
    >>> 104  |    (torch._VF.unique_dim, "aten::unique_dim"),
        105  |    (torch._VF.unique_consecutive, "aten::unique_consecutive"),  # type: ignore[attr-defined]
        106  |    (torch._VF.nuclear_norm, "aten::nuclear_norm"),
        107  |    (torch._VF.frobenius_norm, "aten::frobenius_norm"),

  Error (MYPY) [attr-defined]
    Module has no attribute "nuclear_norm"

        103  |    (torch._VF.norm, "aten::norm"),  # type: ignore[attr-defined]
        104  |    (torch._VF.unique_dim, "aten::unique_dim"),
        105  |    (torch._VF.unique_consecutive, "aten::unique_consecutive"),  # type: ignore[attr-defined]
    >>> 106  |    (torch._VF.nuclear_norm, "aten::nuclear_norm"),
        107  |    (torch._VF.frobenius_norm, "aten::frobenius_norm"),
        108  |    (torch._VF.tensordot, "aten::tensordot"),  # type: ignore[attr-defined]
        109  |]

  Error (MYPY) [attr-defined]
    Module has no attribute "frobenius_norm"

        104  |    (torch._VF.unique_dim, "aten::unique_dim"),
        105  |    (torch._VF.unique_consecutive, "aten::unique_consecutive"),  # type: ignore[attr-defined]
        106  |    (torch._VF.nuclear_norm, "aten::nuclear_norm"),
    >>> 107  |    (torch._VF.frobenius_norm, "aten::frobenius_norm"),
        108  |    (torch._VF.tensordot, "aten::tensordot"),  # type: ignore[attr-defined]
        109  |]
        110  |



>>> Lint for torch/nested/__init__.py:

  Error (MYPY) [attr-defined]
    Module has no attribute "_nested_compute_contiguous_strides_offsets"

         99  |            return torch._nested_view_from_buffer(
        100  |                buffer,
        101  |                nested_sizes,
    >>> 102  |                *torch._nested_compute_contiguous_strides_offsets(nested_sizes))
        103  |        else:
        104  |            assert isinstance(ts, list)
        105  |            return torch._nested_tensor_from_tensor_list(ts, dtype, None, device, None)

  Error (MYPY) [arg-type]
    Argument 2 to "pad" has incompatible type "tuple[int, int]"; expected
    "list[int]"

        379  |        else:
        380  |            # TODO: Truly support offsets=None at some point?
        381  |            # For now, just convert lengths -> offsets for kernel convenience
    >>> 382  |            offsets = F.pad(lengths.cumsum(0), (1, 0))
        383  |            lengths = None
        384  |
        385  |    if jagged_dim is None:



>>> Lint for torch/nested/_internal/nested_tensor.py:

  Error (MYPY) [attr-defined]
    Module has no attribute "_get_nested_int"

         15  |    global _tensor_id_counter
         16  |    tensor_symint = _tensor_symint_registry.get(tensor)
         17  |    if tensor_symint is None:
    >>>  18  |        tensor_symint = torch._C._get_nested_int(_tensor_id_counter, coeff)
         19  |        _tensor_id_counter += 1
         20  |        _tensor_symint_registry[tensor] = tensor_symint
         21  |    return tensor_symint



>>> Lint for torch/nested/_internal/ops.py:

  Error (MYPY) [arg-type]
    Argument 2 to "pad" has incompatible type "tuple[int, int]"; expected
    "list[int]"

         579  |        lengths = inp._offsets.diff()
         580  |        chunked_lengths = lengths.chunk(chunks)
         581  |        chunked_offsets = [torch.cumsum(x, dim=0) for x in chunked_lengths]
    >>>  582  |        chunked_offsets = [F.pad(x, (1, 0), value=0) for x in chunked_offsets]
         583  |        nested_kwargs = [
         584  |            {"offsets": per_offsets, "_ragged_idx": inp._ragged_idx}
         585  |            for per_offsets in chunked_offsets



>>> Lint for torch/nn/functional.py:

  Error (MYPY) [attr-defined]
    Module "torch._jit_internal" has no attribute "BroadcastingList2"; maybe
    "BroadcastingList1"?

          22  |    # The JIT doesn't understand Union, nor torch.dtype here
          23  |    DType = int
          24  |
    >>>   25  |from .._jit_internal import boolean_dispatch, _overload, BroadcastingList1, BroadcastingList2, BroadcastingList3
          26  |from ..overrides import (
          27  |    has_torch_function, has_torch_function_unary, has_torch_function_variadic,
          28  |    handle_torch_function)

  Error (MYPY) [attr-defined]
    Module "torch._jit_internal" has no attribute "BroadcastingList3"; maybe
    "BroadcastingList1"?

          22  |    # The JIT doesn't understand Union, nor torch.dtype here
          23  |    DType = int
          24  |
    >>>   25  |from .._jit_internal import boolean_dispatch, _overload, BroadcastingList1, BroadcastingList2, BroadcastingList3
          26  |from ..overrides import (
          27  |    has_torch_function, has_torch_function_unary, has_torch_function_variadic,
          28  |    handle_torch_function)

  Error (MYPY) [valid-type]
    Variable "torch._jit_internal.BroadcastingList1" is not valid as a type

         636  |
         637  |
         638  |def max_pool1d_with_indices(
    >>>  639  |    input: Tensor, kernel_size: BroadcastingList1[int],
         640  |    stride: Optional[BroadcastingList1[int]] = None,
         641  |    padding: BroadcastingList1[int] = 0,
         642  |    dilation: BroadcastingList1[int] = 1,

  Error (MYPY) [valid-type]
    Variable "torch._jit_internal.BroadcastingList1" is not valid as a type

         637  |
         638  |def max_pool1d_with_indices(
         639  |    input: Tensor, kernel_size: BroadcastingList1[int],
    >>>  640  |    stride: Optional[BroadcastingList1[int]] = None,
         641  |    padding: BroadcastingList1[int] = 0,
         642  |    dilation: BroadcastingList1[int] = 1,
         643  |    ceil_mode: bool = False,

  Error (MYPY) [valid-type]
    Variable "torch._jit_internal.BroadcastingList1" is not valid as a type

         638  |def max_pool1d_with_indices(
         639  |    input: Tensor, kernel_size: BroadcastingList1[int],
         640  |    stride: Optional[BroadcastingList1[int]] = None,
    >>>  641  |    padding: BroadcastingList1[int] = 0,
         642  |    dilation: BroadcastingList1[int] = 1,
         643  |    ceil_mode: bool = False,
         644  |    return_indices: bool = False

  Error (MYPY) [valid-type]
    Variable "torch._jit_internal.BroadcastingList1" is not valid as a type

         639  |    input: Tensor, kernel_size: BroadcastingList1[int],
         640  |    stride: Optional[BroadcastingList1[int]] = None,
         641  |    padding: BroadcastingList1[int] = 0,
    >>>  642  |    dilation: BroadcastingList1[int] = 1,
         643  |    ceil_mode: bool = False,
         644  |    return_indices: bool = False
         645  |) -> Tuple[Tensor, Tensor]:  # noqa: D400

  Error (MYPY) [valid-type]
    Variable "torch._jit_internal.BroadcastingList1" is not valid as a type

         686  |
         687  |
         688  |def _max_pool1d(
    >>>  689  |    input: Tensor, kernel_size: BroadcastingList1[int],
         690  |    stride: Optional[BroadcastingList1[int]] = None,
         691  |    padding: BroadcastingList1[int] = 0,
         692  |    dilation: BroadcastingList1[int] = 1,

  Error (MYPY) [valid-type]
    Variable "torch._jit_internal.BroadcastingList1" is not valid as a type

         687  |
         688  |def _max_pool1d(
         689  |    input: Tensor, kernel_size: BroadcastingList1[int],
    >>>  690  |    stride: Optional[BroadcastingList1[int]] = None,
         691  |    padding: BroadcastingList1[int] = 0,
         692  |    dilation: BroadcastingList1[int] = 1,
         693  |    ceil_mode: bool = False,

  Error (MYPY) [valid-type]
    Variable "torch._jit_internal.BroadcastingList1" is not valid as a type

         688  |def _max_pool1d(
         689  |    input: Tensor, kernel_size: BroadcastingList1[int],
         690  |    stride: Optional[BroadcastingList1[int]] = None,
    >>>  691  |    padding: BroadcastingList1[int] = 0,
         692  |    dilation: BroadcastingList1[int] = 1,
         693  |    ceil_mode: bool = False,
         694  |    return_indices: bool = False

  Error (MYPY) [valid-type]
    Variable "torch._jit_internal.BroadcastingList1" is not valid as a type

         689  |    input: Tensor, kernel_size: BroadcastingList1[int],
         690  |    stride: Optional[BroadcastingList1[int]] = None,
         691  |    padding: BroadcastingList1[int] = 0,
    >>>  692  |    dilation: BroadcastingList1[int] = 1,
         693  |    ceil_mode: bool = False,
         694  |    return_indices: bool = False
         695  |) -> Tensor:

  Error (MYPY) [valid-type]
    Variable "torch._jit_internal.BroadcastingList1" is not valid as a type

         924  |
         925  |def max_unpool1d(
         926  |    input: Tensor, indices: Tensor,
    >>>  927  |    kernel_size: BroadcastingList1[int],
         928  |    stride: Optional[BroadcastingList1[int]] = None,
         929  |    padding: BroadcastingList1[int] = 0,
         930  |    output_size: Optional[BroadcastingList1[int]] = None

  Error (MYPY) [valid-type]
    Variable "torch._jit_internal.BroadcastingList1" is not valid as a type

         925  |def max_unpool1d(
         926  |    input: Tensor, indices: Tensor,
         927  |    kernel_size: BroadcastingList1[int],
    >>>  928  |    stride: Optional[BroadcastingList1[int]] = None,
         929  |    padding: BroadcastingList1[int] = 0,
         930  |    output_size: Optional[BroadcastingList1[int]] = None
         931  |) -> Tensor:

  Error (MYPY) [valid-type]
    Variable "torch._jit_internal.BroadcastingList1" is not valid as a type

         926  |    input: Tensor, indices: Tensor,
         927  |    kernel_size: BroadcastingList1[int],
         928  |    stride: Optional[BroadcastingList1[int]] = None,
    >>>  929  |    padding: BroadcastingList1[int] = 0,
         930  |    output_size: Optional[BroadcastingList1[int]] = None
         931  |) -> Tensor:
         932  |    r"""Compute a partial inverse of :class:`MaxPool1d`.

  Error (MYPY) [valid-type]
    Variable "torch._jit_internal.BroadcastingList1" is not valid as a type

         927  |    kernel_size: BroadcastingList1[int],
         928  |    stride: Optional[BroadcastingList1[int]] = None,
         929  |    padding: BroadcastingList1[int] = 0,
    >>>  930  |    output_size: Optional[BroadcastingList1[int]] = None
         931  |) -> Tensor:
         932  |    r"""Compute a partial inverse of :class:`MaxPool1d`.
         933  |

  Error (MYPY) [valid-type]
    Variable "torch._jit_internal.BroadcastingList1" is not valid as a type

        1079  |def lp_pool1d(
        1080  |    input: Tensor, norm_type: Union[int, float],
        1081  |    kernel_size: int,
    >>> 1082  |    stride: Optional[BroadcastingList1[int]] = None,
        1083  |    ceil_mode: bool = False
        1084  |) -> Tensor:
        1085  |    r"""Apply a 1D power-average pooling over an input signal composed of several input planes.

  Error (MYPY) [valid-type]
    Variable "torch._jit_internal.BroadcastingList1" is not valid as a type

        1102  |
        1103  |
        1104  |def adaptive_max_pool1d_with_indices(
    >>> 1105  |    input: Tensor, output_size: BroadcastingList1[int], return_indices: bool = False
        1106  |) -> Tuple[Tensor, Tensor]:  # noqa: D400
        1107  |    r"""
        1108  |    adaptive_max_pool1d(input, output_size, return_indices=False)

  Error (MYPY) [valid-type]
    Variable "torch._jit_internal.BroadcastingList1" is not valid as a type

        1123  |    return torch.adaptive_max_pool1d(input, output_size)
        1124  |
        1125  |
    >>> 1126  |def _adaptive_max_pool1d(input: Tensor, output_size: BroadcastingList1[int], return_indices: bool = False) -> Tensor:
        1127  |    if has_torch_function_unary(input):
        1128  |        return handle_torch_function(
        1129  |            adaptive_max_pool1d, (input,), input, output_size, return_indices=return_indices

  Error (MYPY) [no-redef]
    Name "upsample" already defined on line 3757

        3759  |    pass
        3760  |
        3761  |
    >>> 3762  |@_overload  # noqa: F811
        3763  |def upsample(input: Tensor, size: Optional[List[int]] = None, scale_factor: Optional[float] = None, mode: str = "nearest", align_corners: Optional[bool] = None) -> Tensor:  # noqa: F811,B950
        3764  |    pass
        3765  |

  Error (MYPY) [no-redef]
    Name "upsample" already defined on line 3757

        3764  |    pass
        3765  |
        3766  |
    >>> 3767  |def upsample(input, size=None, scale_factor=None, mode="nearest", align_corners=None):  # noqa: F811
        3768  |    r"""Upsample input.
        3769  |
        3770  |    Provided tensor is upsampled to either the given :attr:`size` or the given

  Error (MYPY) [no-redef]
    Name "interpolate" already defined on line 3846

        3848  |    pass
        3849  |
        3850  |
    >>> 3851  |@_overload  # noqa: F811
        3852  |def interpolate(input: Tensor, size: Optional[List[int]] = None, scale_factor: Optional[List[float]] = None, mode: str = 'nearest', align_corners: Optional[bool] = None, recompute_scale_factor: Optional[bool] = None, antialias: bool = False) -> Tensor:  # noqa: F811,B950
        3853  |    pass
        3854  |

  Error (MYPY) [no-redef]
    Name "interpolate" already defined on line 3846

        3853  |    pass
        3854  |
        3855  |
    >>> 3856  |@_overload  # noqa: F811
        3857  |def interpolate(input: Tensor, size: Optional[int] = None, scale_factor: Optional[float] = None, mode: str = 'nearest', align_corners: Optional[bool] = None, recompute_scale_factor: Optional[bool] = None, antialias: bool = False) -> Tensor:  # noqa: F811,B950
        3858  |    pass
        3859  |

  Error (MYPY) [no-redef]
    Name "interpolate" already defined on line 3846

        3858  |    pass
        3859  |
        3860  |
    >>> 3861  |@_overload  # noqa: F811
        3862  |def interpolate(  # noqa: F811
        3863  |    input: Tensor,
        3864  |    size: Optional[List[int]] = None,

  Error (MYPY) [no-redef]
    Name "interpolate" already defined on line 3846

        3870  |) -> Tensor:  # noqa: F811
        3871  |    pass
        3872  |
    >>> 3873  |def interpolate(input: Tensor, size: Optional[int] = None, scale_factor: Optional[List[float]] = None, mode: str = 'nearest', align_corners: Optional[bool] = None, recompute_scale_factor: Optional[bool] = None, antialias: bool = False) -> Tensor:  # noqa: F811,B950
        3874  |    r"""Down/up samples the input.
        3875  |
        3876  |    Tensor interpolated to either the given :attr:`size` or the given

  Error (MYPY) [no-redef]
    Name "upsample_nearest" already defined on line 4116

        4118  |    pass
        4119  |
        4120  |
    >>> 4121  |@_overload  # noqa: F811
        4122  |def upsample_nearest(input: Tensor, size: Optional[List[int]] = None, scale_factor: Optional[float] = None) -> Tensor:  # noqa: F811
        4123  |    pass
        4124  |

  Error (MYPY) [no-redef]
    Name "upsample_nearest" already defined on line 4116

        4123  |    pass
        4124  |
        4125  |
    >>> 4126  |def upsample_nearest(input, size=None, scale_factor=None):  # noqa: F811
        4127  |    r"""Upsamples the input, using nearest neighbours' pixel values.
        4128  |
        4129  |    .. warning::

  Error (MYPY) [no-redef]
    Name "upsample_bilinear" already defined on line 4154

        4158  |    pass
        4159  |
        4160  |
    >>> 4161  |@_overload  # noqa: F811
        4162  |def upsample_bilinear(  # noqa: F811
        4163  |    input: Tensor, size: Optional[List[int]] = None, scale_factor: Optional[float] = None
        4164  |) -> Tensor:  # noqa: F811

  Error (MYPY) [no-redef]
    Name "upsample_bilinear" already defined on line 4154

        4165  |    pass
        4166  |
        4167  |
    >>> 4168  |@_overload  # noqa: F811
        4169  |def upsample_bilinear(  # noqa: F811
        4170  |    input: Tensor, size: Optional[int] = None, scale_factor: Optional[List[float]] = None
        4171  |) -> Tensor:  # noqa: F811

  Error (MYPY) [no-redef]
    Name "upsample_bilinear" already defined on line 4154

        4172  |    pass
        4173  |
        4174  |
    >>> 4175  |@_overload  # noqa: F811
        4176  |def upsample_bilinear(  # noqa: F811
        4177  |    input: Tensor, size: Optional[List[int]] = None, scale_factor: Optional[List[float]] = None
        4178  |) -> Tensor:  # noqa: F811

  Error (MYPY) [no-redef]
    Name "upsample_bilinear" already defined on line 4154

        4179  |    pass
        4180  |
        4181  |
    >>> 4182  |def upsample_bilinear(input, size=None, scale_factor=None):  # noqa: F811
        4183  |    r"""Upsamples the input, using bilinear upsampling.
        4184  |
        4185  |    .. warning::

  Error (MYPY) [assignment]
    Incompatible types in assignment (expression has type "None", variable has
    type Module)

           7  |try:
           8  |    import numpy as np
           9  |except ModuleNotFoundError:
    >>>   10  |    np = None
          11  |
          12  |import torch
          13  |from torch import _VF

  Error (MYPY) [arg-type]
    Argument 3 to "max_pool1d_with_indices" has incompatible type
    "BroadcastingList1?[builtins.int] | None"; expected "int | Size |
    list[int] | tuple[int, ...]"

         682  |        )
         683  |    if stride is None:
         684  |        stride = torch.jit.annotate(List[int], [])
    >>>  685  |    return torch.max_pool1d_with_indices(input, kernel_size, stride, padding, dilation, ceil_mode)
         686  |
         687  |
         688  |def _max_pool1d(

  Error (MYPY) [arg-type]
    Argument 3 to "max_pool1d" has incompatible type "BroadcastingList1?
    [builtins.int] | None"; expected "int | Size | list[int] |
    tuple[int, ...]"

         707  |        )
         708  |    if stride is None:
         709  |        stride = torch.jit.annotate(List[int], [])
    >>>  710  |    return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)
         711  |
         712  |
         713  |max_pool1d = boolean_dispatch(

  Error (MYPY) [attr-defined]
    Module has no attribute "max_pool2d_with_indices"

         768  |        )
         769  |    if stride is None:
         770  |        stride = torch.jit.annotate(List[int], [])
    >>>  771  |    return torch._C._nn.max_pool2d_with_indices(input, kernel_size, stride, padding, dilation, ceil_mode)
         772  |
         773  |
         774  |def _max_pool2d(

  Error (MYPY) [attr-defined]
    Module has no attribute "max_pool3d_with_indices"

         854  |        )
         855  |    if stride is None:
         856  |        stride = torch.jit.annotate(List[int], [])
    >>>  857  |    return torch._C._nn.max_pool3d_with_indices(input, kernel_size, stride, padding, dilation, ceil_mode)
         858  |
         859  |
         860  |def _max_pool3d(

  Error (MYPY) [attr-defined]
    Module has no attribute "max_unpool2d"

         955  |        output_size = output_size + [1]
         956  |    else:
         957  |        output_size = output_size + (1,)
    >>>  958  |    return torch._C._nn.max_unpool2d(input.unsqueeze(-1), indices.unsqueeze(-1), output_size).squeeze(-1)
         959  |
         960  |
         961  |def max_unpool2d(

  Error (MYPY) [attr-defined]
    Module has no attribute "max_unpool2d"

         987  |        _stride = kernel_size
         988  |    padding = _pair(padding)
         989  |    output_size = _unpool_output_size(input, kernel_size, _stride, padding, output_size)
    >>>  990  |    return torch._C._nn.max_unpool2d(input, indices, output_size)
         991  |
         992  |
         993  |def max_unpool3d(

  Error (MYPY) [attr-defined]
    Module has no attribute "max_unpool3d"

        1019  |        _stride = kernel_size
        1020  |    padding = _triple(padding)
        1021  |    output_size = _unpool_output_size(input, kernel_size, _stride, padding, output_size)
    >>> 1022  |    return torch._C._nn.max_unpool3d(input, indices, output_size, _stride, padding)
        1023  |
        1024  |
        1025  |def lp_pool3d(

  Error (MYPY) [arg-type]
    Argument 2 to "_list_with_default" has incompatible type "Size"; expected
    "list[int]"

        1162  |        return handle_torch_function(
        1163  |            adaptive_max_pool2d_with_indices, (input,), input, output_size, return_indices=return_indices
        1164  |        )
    >>> 1165  |    output_size = _list_with_default(output_size, input.size())
        1166  |    return torch._C._nn.adaptive_max_pool2d(input, output_size)
        1167  |
        1168  |

  Error (MYPY) [arg-type]
    Argument 2 to "_list_with_default" has incompatible type "Size"; expected
    "list[int]"

        1206  |        return handle_torch_function(
        1207  |            adaptive_max_pool3d_with_indices, (input,), input, output_size, return_indices=return_indices
        1208  |        )
    >>> 1209  |    output_size = _list_with_default(output_size, input.size())
        1210  |    return torch._C._nn.adaptive_max_pool3d(input, output_size)
        1211  |
        1212  |

  Error (MYPY) [arg-type]
    Argument 2 to "_list_with_default" has incompatible type "Size"; expected
    "list[int]"

        1256  |    """
        1257  |    if has_torch_function_unary(input):
        1258  |        return handle_torch_function(adaptive_avg_pool2d, (input,), input, output_size)
    >>> 1259  |    _output_size = _list_with_default(output_size, input.size())
        1260  |    return torch._C._nn.adaptive_avg_pool2d(input, _output_size)
        1261  |
        1262  |

  Error (MYPY) [attr-defined]
    Module has no attribute "adaptive_avg_pool2d"; maybe "adaptive_max_pool2d"
    or "adaptive_max_pool3d"?

        1257  |    if has_torch_function_unary(input):
        1258  |        return handle_torch_function(adaptive_avg_pool2d, (input,), input, output_size)
        1259  |    _output_size = _list_with_default(output_size, input.size())
    >>> 1260  |    return torch._C._nn.adaptive_avg_pool2d(input, _output_size)
        1261  |
        1262  |
        1263  |def adaptive_avg_pool3d(input: Tensor, output_size: BroadcastingList3[int]) -> Tensor:

  Error (MYPY) [arg-type]
    Argument 2 to "_list_with_default" has incompatible type "Size"; expected
    "list[int]"

        1271  |    """
        1272  |    if has_torch_function_unary(input):
        1273  |        return handle_torch_function(adaptive_avg_pool3d, (input,), input, output_size)
    >>> 1274  |    _output_size = _list_with_default(output_size, input.size())
        1275  |    return torch._C._nn.adaptive_avg_pool3d(input, _output_size)
        1276  |
        1277  |

  Error (MYPY) [attr-defined]
    Module has no attribute "adaptive_avg_pool3d"; maybe "adaptive_max_pool3d"
    or "adaptive_max_pool2d"?

        1272  |    if has_torch_function_unary(input):
        1273  |        return handle_torch_function(adaptive_avg_pool3d, (input,), input, output_size)
        1274  |    _output_size = _list_with_default(output_size, input.size())
    >>> 1275  |    return torch._C._nn.adaptive_avg_pool3d(input, _output_size)
        1276  |
        1277  |
        1278  |# Activation functions

  Error (MYPY) [attr-defined]
    Module has no attribute "dropout_"

        1292  |        return handle_torch_function(dropout, (input,), input, p=p, training=training, inplace=inplace)
        1293  |    if p < 0.0 or p > 1.0:
        1294  |        raise ValueError(f"dropout probability has to be between 0 and 1, but got {p}")
    >>> 1295  |    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
        1296  |
        1297  |
        1298  |def alpha_dropout(input: Tensor, p: float = 0.5, training: bool = False, inplace: bool = False) -> Tensor:

  Error (MYPY) [attr-defined]
    Module has no attribute "dropout"

        1292  |        return handle_torch_function(dropout, (input,), input, p=p, training=training, inplace=inplace)
        1293  |    if p < 0.0 or p > 1.0:
        1294  |        raise ValueError(f"dropout probability has to be between 0 and 1, but got {p}")
    >>> 1295  |    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
        1296  |
        1297  |
        1298  |def alpha_dropout(input: Tensor, p: float = 0.5, training: bool = False, inplace: bool = False) -> Tensor:

  Error (MYPY) [attr-defined]
    Module has no attribute "alpha_dropout_"

        1304  |        return handle_torch_function(alpha_dropout, (input,), input, p=p, training=training, inplace=inplace)
        1305  |    if p < 0.0 or p > 1.0:
        1306  |        raise ValueError(f"dropout probability has to be between 0 and 1, but got {p}")
    >>> 1307  |    return _VF.alpha_dropout_(input, p, training) if inplace else _VF.alpha_dropout(input, p, training)
        1308  |
        1309  |
        1310  |def dropout1d(input: Tensor, p: float = 0.5, training: bool = True, inplace: bool = False) -> Tensor:

  Error (MYPY) [attr-defined]
    Module has no attribute "alpha_dropout"

        1304  |        return handle_torch_function(alpha_dropout, (input,), input, p=p, training=training, inplace=inplace)
        1305  |    if p < 0.0 or p > 1.0:
        1306  |        raise ValueError(f"dropout probability has to be between 0 and 1, but got {p}")
    >>> 1307  |    return _VF.alpha_dropout_(input, p, training) if inplace else _VF.alpha_dropout(input, p, training)
        1308  |
        1309  |
        1310  |def dropout1d(input: Tensor, p: float = 0.5, training: bool = True, inplace: bool = False) -> Tensor:

  Error (MYPY) [attr-defined]
    Module has no attribute "feature_dropout_"

        1337  |    if not is_batched:
        1338  |        input = input.unsqueeze_(0) if inplace else input.unsqueeze(0)
        1339  |
    >>> 1340  |    result = _VF.feature_dropout_(input, p, training) if inplace else _VF.feature_dropout(input, p, training)
        1341  |
        1342  |    if not is_batched:
        1343  |        result = result.squeeze_(0) if inplace else result.squeeze(0)

  Error (MYPY) [attr-defined]
    Module has no attribute "feature_dropout"

        1337  |    if not is_batched:
        1338  |        input = input.unsqueeze_(0) if inplace else input.unsqueeze(0)
        1339  |
    >>> 1340  |    result = _VF.feature_dropout_(input, p, training) if inplace else _VF.feature_dropout(input, p, training)
        1341  |
        1342  |    if not is_batched:
        1343  |        result = result.squeeze_(0) if inplace else result.squeeze(0)

  Error (MYPY) [attr-defined]
    Module has no attribute "feature_dropout_"

        1384  |                      "input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D "
        1385  |                      "channel-wise dropout behavior, please switch to using dropout1d instead.")
        1386  |
    >>> 1387  |    result = _VF.feature_dropout_(input, p, training) if inplace else _VF.feature_dropout(input, p, training)
        1388  |
        1389  |    return result
        1390  |

  Error (MYPY) [attr-defined]
    Module has no attribute "feature_dropout"

        1384  |                      "input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D "
        1385  |                      "channel-wise dropout behavior, please switch to using dropout1d instead.")
        1386  |
    >>> 1387  |    result = _VF.feature_dropout_(input, p, training) if inplace else _VF.feature_dropout(input, p, training)
        1388  |
        1389  |    return result
        1390  |

  Error (MYPY) [attr-defined]
    Module has no attribute "feature_dropout_"

        1421  |    if not is_batched:
        1422  |        input = input.unsqueeze_(0) if inplace else input.unsqueeze(0)
        1423  |
    >>> 1424  |    result = _VF.feature_dropout_(input, p, training) if inplace else _VF.feature_dropout(input, p, training)
        1425  |
        1426  |    if not is_batched:
        1427  |        result = result.squeeze_(0) if inplace else result.squeeze(0)

  Error (MYPY) [attr-defined]
    Module has no attribute "feature_dropout"

        1421  |    if not is_batched:
        1422  |        input = input.unsqueeze_(0) if inplace else input.unsqueeze(0)
        1423  |
    >>> 1424  |    result = _VF.feature_dropout_(input, p, training) if inplace else _VF.feature_dropout(input, p, training)
        1425  |
        1426  |    if not is_batched:
        1427  |        result = result.squeeze_(0) if inplace else result.squeeze(0)

  Error (MYPY) [attr-defined]
    Module has no attribute "feature_alpha_dropout_"

        1454  |        )
        1455  |    if p < 0.0 or p > 1.0:
        1456  |        raise ValueError(f"dropout probability has to be between 0 and 1, but got {p}")
    >>> 1457  |    return _VF.feature_alpha_dropout_(input, p, training) if inplace else _VF.feature_alpha_dropout(input, p, training)
        1458  |
        1459  |
        1460  |def _threshold(input: Tensor, threshold: float, value: float, inplace: bool = False) -> Tensor:

  Error (MYPY) [attr-defined]
    Module has no attribute "feature_alpha_dropout"

        1454  |        )
        1455  |    if p < 0.0 or p > 1.0:
        1456  |        raise ValueError(f"dropout probability has to be between 0 and 1, but got {p}")
    >>> 1457  |    return _VF.feature_alpha_dropout_(input, p, training) if inplace else _VF.feature_alpha_dropout(input, p, training)
        1458  |
        1459  |
        1460  |def _threshold(input: Tensor, threshold: float, value: float, inplace: bool = False) -> Tensor:

  Error (MYPY) [attr-defined]
    Module has no attribute "threshold_"

        1465  |    if has_torch_function_unary(input):
        1466  |        return handle_torch_function(_threshold, (input,), input, threshold, value, inplace=inplace)
        1467  |    if inplace:
    >>> 1468  |        result = _VF.threshold_(input, threshold, value)
        1469  |    else:
        1470  |        result = _VF.threshold(input, threshold, value)
        1471  |    return result

  Error (MYPY) [attr-defined]
    Module has no attribute "threshold"

        1467  |    if inplace:
        1468  |        result = _VF.threshold_(input, threshold, value)
        1469  |    else:
    >>> 1470  |        result = _VF.threshold(input, threshold, value)
        1471  |    return result
        1472  |
        1473  |

  Error (MYPY) [attr-defined]
    Module has no attribute "threshold_"

        1477  |threshold = _threshold
        1478  |
        1479  |threshold_ = _add_docstr(
    >>> 1480  |    _VF.threshold_,
        1481  |    r"""
        1482  |threshold_(input, threshold, value) -> Tensor
        1483  |

  Error (MYPY) [attr-defined]
    Module has no attribute "glu"; maybe "gelu"?

        1533  |        return handle_torch_function(glu, (input,), input, dim=dim)
        1534  |    if input.dim() == 0:
        1535  |        raise RuntimeError("glu does not support scalars because halving size must be even")
    >>> 1536  |    return torch._C._nn.glu(input, dim)
        1537  |
        1538  |
        1539  |def hardtanh(input: Tensor, min_val: float = -1., max_val: float = 1., inplace: bool = False) -> Tensor:  # noqa: D400,D402

  Error (MYPY) [attr-defined]
    Module has no attribute "relu6_"; maybe "elu_"?

        1574  |    if has_torch_function_unary(input):
        1575  |        return handle_torch_function(relu6, (input,), input, inplace=inplace)
        1576  |    if inplace:
    >>> 1577  |        result = torch._C._nn.relu6_(input)
        1578  |    else:
        1579  |        result = torch._C._nn.relu6(input)
        1580  |    return result

  Error (MYPY) [attr-defined]
    Module has no attribute "relu6"

        1576  |    if inplace:
        1577  |        result = torch._C._nn.relu6_(input)
        1578  |    else:
    >>> 1579  |        result = torch._C._nn.relu6(input)
        1580  |    return result
        1581  |
        1582  |

  Error (MYPY) [attr-defined]
    Module has no attribute "elu"; maybe "elu_" or "gelu"?

        1590  |    if inplace:
        1591  |        result = torch._C._nn.elu_(input, alpha)
        1592  |    else:
    >>> 1593  |        result = torch._C._nn.elu(input, alpha)
        1594  |    return result
        1595  |
        1596  |

  Error (MYPY) [attr-defined]
    Module has no attribute "hardsigmoid_"; maybe "hardsigmoid"?

        2030  |    if has_torch_function_unary(input):
        2031  |        return handle_torch_function(hardsigmoid, (input,), input, inplace=inplace)
        2032  |    if inplace:
    >>> 2033  |        return torch._C._nn.hardsigmoid_(input)
        2034  |    return torch._C._nn.hardsigmoid(input)
        2035  |
        2036  |

  Error (MYPY) [attr-defined]
    Module has no attribute "silu_"

        2100  |    if has_torch_function_unary(input):
        2101  |        return handle_torch_function(silu, (input,), input, inplace=inplace)
        2102  |    if inplace:
    >>> 2103  |        return torch._C._nn.silu_(input)
        2104  |    return torch._C._nn.silu(input)
        2105  |
        2106  |

  Error (MYPY) [attr-defined]
    Module has no attribute "silu"

        2101  |        return handle_torch_function(silu, (input,), input, inplace=inplace)
        2102  |    if inplace:
        2103  |        return torch._C._nn.silu_(input)
    >>> 2104  |    return torch._C._nn.silu(input)
        2105  |
        2106  |
        2107  |def mish(input: Tensor, inplace: bool = False) -> Tensor:

  Error (MYPY) [attr-defined]
    Module has no attribute "mish_"

        2120  |    if has_torch_function_unary(input):
        2121  |        return handle_torch_function(mish, (input,), input, inplace=inplace)
        2122  |    if inplace:
    >>> 2123  |        return torch._C._nn.mish_(input)
        2124  |    return torch._C._nn.mish(input)
        2125  |
        2126  |

  Error (MYPY) [attr-defined]
    Module has no attribute "mish"

        2121  |        return handle_torch_function(mish, (input,), input, inplace=inplace)
        2122  |    if inplace:
        2123  |        return torch._C._nn.mish_(input)
    >>> 2124  |    return torch._C._nn.mish(input)
        2125  |
        2126  |
        2127  |def hardswish(input: Tensor, inplace: bool = False) -> Tensor:

  Error (MYPY) [attr-defined]
    Module has no attribute "hardswish_"

        2145  |    if has_torch_function_unary(input):
        2146  |        return handle_torch_function(hardswish, (input,), input, inplace=inplace)
        2147  |    if inplace:
    >>> 2148  |        return torch._C._nn.hardswish_(input)
        2149  |    return torch._C._nn.hardswish(input)
        2150  |
        2151  |

  Error (MYPY) [attr-defined]
    Module has no attribute "hardswish"

        2146  |        return handle_torch_function(hardswish, (input,), input, inplace=inplace)
        2147  |    if inplace:
        2148  |        return torch._C._nn.hardswish_(input)
    >>> 2149  |    return torch._C._nn.hardswish(input)
        2150  |
        2151  |
        2152  |def _no_grad_embedding_renorm_(weight: Tensor, input: Tensor, max_norm: float, norm_type: float) -> Tuple[Tensor, Tensor]:

  Error (MYPY) [return]
    Missing return statement

        2149  |    return torch._C._nn.hardswish(input)
        2150  |
        2151  |
    >>> 2152  |def _no_grad_embedding_renorm_(weight: Tensor, input: Tensor, max_norm: float, norm_type: float) -> Tuple[Tensor, Tensor]:
        2153  |    torch.embedding_renorm_(weight.detach(), input, max_norm, norm_type)
        2154  |
        2155  |

  Error (MYPY) [arg-type]
    Argument 1 to "_verify_batch_size" has incompatible type "Size"; expected
    "list[int]"

        2506  |            eps=eps,
        2507  |        )
        2508  |    if training:
    >>> 2509  |        _verify_batch_size(input.size())
        2510  |
        2511  |    return torch.batch_norm(
        2512  |        input, weight, bias, running_mean, running_var, training, momentum, eps, torch.backends.cudnn.enabled

  Error (MYPY) [arg-type]
    Argument 1 to "_verify_spatial_size" has incompatible type "Size";
    expected "list[int]"

        2551  |            eps=eps,
        2552  |        )
        2553  |    if use_input_stats:
    >>> 2554  |        _verify_spatial_size(input.size())
        2555  |    return torch.instance_norm(
        2556  |        input, weight, bias, running_mean, running_var, use_input_stats, momentum, eps, torch.backends.cudnn.enabled
        2557  |    )

  Error (MYPY) [attr-defined]
    Module has no attribute "rms_norm"

        2588  |        return handle_torch_function(
        2589  |            rms_norm, (input, weight), input, normalized_shape, weight=weight, eps=eps
        2590  |        )
    >>> 2591  |    return torch.rms_norm(input, normalized_shape, weight, eps)
        2592  |
        2593  |def group_norm(
        2594  |    input: Tensor, num_groups: int, weight: Optional[Tensor] = None, bias: Optional[Tensor] = None, eps: float = 1e-5

  Error (MYPY) [arg-type]
    Argument 2 to "pad" has incompatible type "tuple[int, int, int, int]";
    expected "list[int]"

        2627  |    div = input.mul(input)
        2628  |    if dim == 3:
        2629  |        div = div.unsqueeze(1)
    >>> 2630  |        div = pad(div, (0, 0, size // 2, (size - 1) // 2))
        2631  |        div = avg_pool2d(div, (size, 1), stride=1).squeeze(1)
        2632  |    else:
        2633  |        sizes = input.size()

  Error (MYPY) [arg-type]
    Argument 2 to "pad" has incompatible type "tuple[int, int, int, int, int,
    int]"; expected "list[int]"

        2632  |    else:
        2633  |        sizes = input.size()
        2634  |        div = div.view(sizes[0], 1, sizes[1], sizes[2], -1)
    >>> 2635  |        div = pad(div, (0, 0, 0, 0, size // 2, (size - 1) // 2))
        2636  |        div = avg_pool3d(div, (size, 1, 1), stride=1).squeeze(1)
        2637  |        div = div.view(sizes)
        2638  |    div = div.mul(alpha).add(k).pow(beta)

  Error (MYPY) [attr-defined]
    Module has no attribute "nll_loss_nd"

        2774  |        )
        2775  |    if size_average is not None or reduce is not None:
        2776  |        reduction = _Reduction.legacy_get_string(size_average, reduce)
    >>> 2777  |    return torch._C._nn.nll_loss_nd(input, target, weight, _Reduction.get_enum(reduction), ignore_index)
        2778  |
        2779  |
        2780  |def poisson_nll_loss(

  Error (MYPY) [attr-defined]
    Module has no attribute "cross_entropy_loss"

        3100  |        )
        3101  |    if size_average is not None or reduce is not None:
        3102  |        reduction = _Reduction.legacy_get_string(size_average, reduce)
    >>> 3103  |    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)
        3104  |
        3105  |
        3106  |def binary_cross_entropy(

  Error (MYPY) [attr-defined]
    Module has no attribute "binary_cross_entropy"

        3168  |        new_size = _infer_size(target.size(), weight.size())
        3169  |        weight = weight.expand(new_size)
        3170  |
    >>> 3171  |    return torch._C._nn.binary_cross_entropy(input, target, weight, reduction_enum)
        3172  |
        3173  |
        3174  |def binary_cross_entropy_with_logits(

  Error (MYPY) [attr-defined]
    Module has no attribute "l1_loss"

        3282  |    expanded_input, expanded_target = torch.broadcast_tensors(input, target)
        3283  |
        3284  |    if beta == 0.0:
    >>> 3285  |        return torch._C._nn.l1_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction))
        3286  |    else:
        3287  |        return torch._C._nn.smooth_l1_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction), beta)
        3288  |

  Error (MYPY) [attr-defined]
    Module has no attribute "smooth_l1_loss"

        3284  |    if beta == 0.0:
        3285  |        return torch._C._nn.l1_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction))
        3286  |    else:
    >>> 3287  |        return torch._C._nn.smooth_l1_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction), beta)
        3288  |
        3289  |
        3290  |def huber_loss(

  Error (MYPY) [attr-defined]
    Module has no attribute "huber_loss"

        3319  |                      stacklevel=2)
        3320  |
        3321  |    expanded_input, expanded_target = torch.broadcast_tensors(input, target)
    >>> 3322  |    return torch._C._nn.huber_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction), delta)
        3323  |
        3324  |
        3325  |def l1_loss(

  Error (MYPY) [attr-defined]
    Module has no attribute "l1_loss"

        3350  |        reduction = _Reduction.legacy_get_string(size_average, reduce)
        3351  |
        3352  |    expanded_input, expanded_target = torch.broadcast_tensors(input, target)
    >>> 3353  |    return torch._C._nn.l1_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction))
        3354  |
        3355  |
        3356  |def mse_loss(

  Error (MYPY) [attr-defined]
    Module has no attribute "mse_loss"

        3380  |        reduction = _Reduction.legacy_get_string(size_average, reduce)
        3381  |
        3382  |    expanded_input, expanded_target = torch.broadcast_tensors(input, target)
    >>> 3383  |    return torch._C._nn.mse_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction))
        3384  |
        3385  |
        3386  |def margin_ranking_loss(

  Error (MYPY) [attr-defined]
    Module has no attribute "multilabel_margin_loss"

        3475  |        reduction_enum = _Reduction.legacy_get_enum(size_average, reduce)
        3476  |    else:
        3477  |        reduction_enum = _Reduction.get_enum(reduction)
    >>> 3478  |    return torch._C._nn.multilabel_margin_loss(input, target, reduction_enum)
        3479  |
        3480  |
        3481  |def soft_margin_loss(

  Error (MYPY) [attr-defined]
    Module has no attribute "soft_margin_loss"

        3498  |        reduction_enum = _Reduction.legacy_get_enum(size_average, reduce)
        3499  |    else:
        3500  |        reduction_enum = _Reduction.get_enum(reduction)
    >>> 3501  |    return torch._C._nn.soft_margin_loss(input, target, reduction_enum)
        3502  |
        3503  |
        3504  |def multilabel_soft_margin_loss(

  Error (MYPY) [attr-defined]
    Module has no attribute "multi_margin_loss"

        3617  |        if weight.dim() != 1:
        3618  |            raise ValueError("weight must be one-dimensional")
        3619  |
    >>> 3620  |    return torch._C._nn.multi_margin_loss(input, target, p, margin, weight, reduction_enum)
        3621  |
        3622  |
        3623  |pixel_shuffle = _add_docstr(

  Error (MYPY) [attr-defined]
    Module has no attribute "im2col"

        4837  |        return handle_torch_function(
        4838  |            unfold, (input,), input, kernel_size, dilation=dilation, padding=padding, stride=stride
        4839  |        )
    >>> 4840  |    return torch._C._nn.im2col(input, _pair(kernel_size), _pair(dilation), _pair(padding), _pair(stride))
        4841  |
        4842  |
        4843  |def fold(

  Error (MYPY) [attr-defined]
    Module has no attribute "col2im"

        4858  |        return handle_torch_function(
        4859  |            fold, (input,), input, output_size, kernel_size, dilation=dilation, padding=padding, stride=stride
        4860  |        )
    >>> 4861  |    return torch._C._nn.col2im(
        4862  |        input, _pair(output_size), _pair(kernel_size), _pair(dilation), _pair(padding), _pair(stride)
        4863  |    )
        4864  |

  Error (MYPY) [return-value]
    Incompatible return value type (got "tuple[Any, Any, Any]", expected
    "list[Tensor]")

        4907  |            proj = linear(q, w, b)
        4908  |            # reshape to 3, E and not E, 3 is deliberate for better memory coalescing and keeping same order as chunk()
        4909  |            proj = proj.unflatten(-1, (3, E)).unsqueeze(0).transpose(0, -2).squeeze(-2).contiguous()
    >>> 4910  |            return proj[0], proj[1], proj[2]
        4911  |        else:
        4912  |            # encoder-decoder attention
        4913  |            w_q, w_kv = w.split([E, E * 2])

  Error (MYPY) [return-value]
    Incompatible return value type (got "tuple[Tensor, Any, Any]", expected
    "list[Tensor]")

        4919  |            kv_proj = linear(k, w_kv, b_kv)
        4920  |            # reshape to 2, E and not E, 2 is deliberate for better memory coalescing and keeping same order as chunk()
        4921  |            kv_proj = kv_proj.unflatten(-1, (2, E)).unsqueeze(0).transpose(0, -2).squeeze(-2).contiguous()
    >>> 4922  |            return (q_proj, kv_proj[0], kv_proj[1])
        4923  |    else:
        4924  |        w_q, w_k, w_v = w.chunk(3)
        4925  |        if b is None:

  Error (MYPY) [return-value]
    Incompatible return value type (got "tuple[Tensor, Tensor, Tensor]",
    expected "list[Tensor]")

        4926  |            b_q = b_k = b_v = None
        4927  |        else:
        4928  |            b_q, b_k, b_v = b.chunk(3)
    >>> 4929  |        return linear(q, w_q, b_q), linear(k, w_k, b_k), linear(v, w_v, b_v)
        4930  |
        4931  |
        4932  |def _in_projection(

  Error (MYPY) [arg-type]
    Argument 2 to "pad" has incompatible type "tuple[int, int]"; expected
    "list[int]"

        5418  |        k = torch.cat([k, bias_k.repeat(1, bsz, 1)])
        5419  |        v = torch.cat([v, bias_v.repeat(1, bsz, 1)])
        5420  |        if attn_mask is not None:
    >>> 5421  |            attn_mask = pad(attn_mask, (0, 1))
        5422  |        if key_padding_mask is not None:
        5423  |            key_padding_mask = pad(key_padding_mask, (0, 1))
        5424  |    else:

  Error (MYPY) [arg-type]
    Argument 2 to "pad" has incompatible type "tuple[int, int]"; expected
    "list[int]"

        5420  |        if attn_mask is not None:
        5421  |            attn_mask = pad(attn_mask, (0, 1))
        5422  |        if key_padding_mask is not None:
    >>> 5423  |            key_padding_mask = pad(key_padding_mask, (0, 1))
        5424  |    else:
        5425  |        assert bias_k is None
        5426  |        assert bias_v is None

  Error (MYPY) [arg-type]
    Argument 2 to "pad" has incompatible type "tuple[int, int]"; expected
    "list[int]"

        5454  |        k = torch.cat([k, torch.zeros(zero_attn_shape, dtype=k.dtype, device=k.device)], dim=1)
        5455  |        v = torch.cat([v, torch.zeros(zero_attn_shape, dtype=v.dtype, device=v.device)], dim=1)
        5456  |        if attn_mask is not None:
    >>> 5457  |            attn_mask = pad(attn_mask, (0, 1))
        5458  |        if key_padding_mask is not None:
        5459  |            key_padding_mask = pad(key_padding_mask, (0, 1))
        5460  |

  Error (MYPY) [arg-type]
    Argument 2 to "pad" has incompatible type "tuple[int, int]"; expected
    "list[int]"

        5456  |        if attn_mask is not None:
        5457  |            attn_mask = pad(attn_mask, (0, 1))
        5458  |        if key_padding_mask is not None:
    >>> 5459  |            key_padding_mask = pad(key_padding_mask, (0, 1))
        5460  |
        5461  |    # update source sequence length after adjustments
        5462  |    src_len = k.size(1)



>>> Lint for torch/nn/modules/activation.py:

  Error (MYPY) [arg-type]
    Argument 2 has incompatible type "float"; expected "int"

         856  |        self.threshold = threshold
         857  |
         858  |    def forward(self, input: Tensor) -> Tensor:
    >>>  859  |        return F.softplus(input, self.beta, self.threshold)
         860  |
         861  |    def extra_repr(self) -> str:
         862  |        return f'beta={self.beta}, threshold={self.threshold}'

  Error (MYPY) [arg-type]
    Argument 3 has incompatible type "float"; expected "int"

         856  |        self.threshold = threshold
         857  |
         858  |    def forward(self, input: Tensor) -> Tensor:
    >>>  859  |        return F.softplus(input, self.beta, self.threshold)
         860  |
         861  |    def extra_repr(self) -> str:
         862  |        return f'beta={self.beta}, threshold={self.threshold}'



>>> Lint for torch/nn/modules/normalization.py:

  Error (MYPY) [arg-type]
    Argument 2 to "layer_norm" has incompatible type "tuple[int, ...]";
    expected "list[int]"

        199  |
        200  |    def forward(self, input: Tensor) -> Tensor:
        201  |        return F.layer_norm(
    >>> 202  |            input, self.normalized_shape, self.weight, self.bias, self.eps)
        203  |
        204  |    def extra_repr(self) -> str:
        205  |        return '{normalized_shape}, eps={eps}, ' \

  Error (MYPY) [arg-type]
    Argument 2 to "rms_norm" has incompatible type "tuple[int, ...]"; expected
    "list[int]"

        364  |        """
        365  |        Runs forward pass.
        366  |        """
    >>> 367  |        return F.rms_norm(x, self.normalized_shape, self.weight, self.eps)
        368  |
        369  |    def extra_repr(self) -> str:
        370  |        """



>>> Lint for torch/nn/modules/padding.py:

  Error (MYPY) [arg-type]
    Argument 2 to "pad" has incompatible type "Sequence[int]"; expected
    "list[int]"

         23  |
         24  |    def forward(self, input: Tensor) -> Tensor:
         25  |        self._check_input_dim(input)
    >>>  26  |        return F.pad(input, self.padding, 'circular')
         27  |
         28  |    def extra_repr(self) -> str:
         29  |        return f'{self.padding}'

  Error (MYPY) [arg-type]
    Argument 2 to "pad" has incompatible type "Sequence[int]"; expected
    "list[int]"

        202  |        self.value = value
        203  |
        204  |    def forward(self, input: Tensor) -> Tensor:
    >>> 205  |        return F.pad(input, self.padding, 'constant', self.value)
        206  |
        207  |    def extra_repr(self) -> str:
        208  |        return f'padding={self.padding}, value={self.value}'

  Error (MYPY) [arg-type]
    Argument 2 to "pad" has incompatible type "Sequence[int]"; expected
    "list[int]"

        355  |    padding: Sequence[int]
        356  |
        357  |    def forward(self, input: Tensor) -> Tensor:
    >>> 358  |        return F.pad(input, self.padding, 'reflect')
        359  |
        360  |    def extra_repr(self) -> str:
        361  |        return f'{self.padding}'

  Error (MYPY) [arg-type]
    Argument 2 to "pad" has incompatible type "Sequence[int]"; expected
    "list[int]"

        514  |    padding: Sequence[int]
        515  |
        516  |    def forward(self, input: Tensor) -> Tensor:
    >>> 517  |        return F.pad(input, self.padding, 'replicate')
        518  |
        519  |    def extra_repr(self) -> str:
        520  |        return f'{self.padding}'



>>> Lint for torch/nn/modules/rnn.py:

  Error (MYPY) [attr-defined]
    Module has no attribute "rnn_tanh"

          15  |__all__ = ['RNNBase', 'RNN', 'LSTM', 'GRU', 'RNNCellBase', 'RNNCell', 'LSTMCell', 'GRUCell']
          16  |
          17  |_rnn_impls = {
    >>>   18  |    'RNN_TANH': _VF.rnn_tanh,
          19  |    'RNN_RELU': _VF.rnn_relu,
          20  |}
          21  |

  Error (MYPY) [attr-defined]
    Module has no attribute "rnn_relu"

          16  |
          17  |_rnn_impls = {
          18  |    'RNN_TANH': _VF.rnn_tanh,
    >>>   19  |    'RNN_RELU': _VF.rnn_relu,
          20  |}
          21  |
          22  |

  Error (MYPY) [attr-defined]
    Module has no attribute "rnn_tanh"

         584  |        assert self.mode == 'RNN_TANH' or self.mode == 'RNN_RELU'
         585  |        if batch_sizes is None:
         586  |            if self.mode == 'RNN_TANH':
    >>>  587  |                result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
         588  |                                      self.dropout, self.training, self.bidirectional,
         589  |                                      self.batch_first)
         590  |            else:

  Error (MYPY) [attr-defined]
    Module has no attribute "rnn_relu"

         588  |                                      self.dropout, self.training, self.bidirectional,
         589  |                                      self.batch_first)
         590  |            else:
    >>>  591  |                result = _VF.rnn_relu(input, hx, self._flat_weights, self.bias, self.num_layers,
         592  |                                      self.dropout, self.training, self.bidirectional,
         593  |                                      self.batch_first)
         594  |        else:

  Error (MYPY) [attr-defined]
    Module has no attribute "rnn_tanh"

         593  |                                      self.batch_first)
         594  |        else:
         595  |            if self.mode == 'RNN_TANH':
    >>>  596  |                result = _VF.rnn_tanh(input, batch_sizes, hx, self._flat_weights, self.bias,
         597  |                                      self.num_layers, self.dropout, self.training,
         598  |                                      self.bidirectional)
         599  |            else:

  Error (MYPY) [attr-defined]
    Module has no attribute "rnn_relu"

         597  |                                      self.num_layers, self.dropout, self.training,
         598  |                                      self.bidirectional)
         599  |            else:
    >>>  600  |                result = _VF.rnn_relu(input, batch_sizes, hx, self._flat_weights, self.bias,
         601  |                                      self.num_layers, self.dropout, self.training,
         602  |                                      self.bidirectional)
         603  |

  Error (MYPY) [attr-defined]
    Module has no attribute "lstm"

         909  |                hx = self.permute_hidden(hx, sorted_indices)
         910  |
         911  |        if batch_sizes is None:
    >>>  912  |            result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,
         913  |                              self.dropout, self.training, self.bidirectional, self.batch_first)
         914  |        else:
         915  |            result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,

  Error (MYPY) [attr-defined]
    Module has no attribute "lstm"

         912  |            result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,
         913  |                              self.dropout, self.training, self.bidirectional, self.batch_first)
         914  |        else:
    >>>  915  |            result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,
         916  |                              self.num_layers, self.dropout, self.training, self.bidirectional)
         917  |        output = result[0]
         918  |        hidden = result[1:]

  Error (MYPY) [attr-defined]
    Module has no attribute "gru"

        1131  |
        1132  |        self.check_forward_args(input, hx, batch_sizes)
        1133  |        if batch_sizes is None:
    >>> 1134  |            result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,
        1135  |                             self.dropout, self.training, self.bidirectional, self.batch_first)
        1136  |        else:
        1137  |            result = _VF.gru(input, batch_sizes, hx, self._flat_weights, self.bias,

  Error (MYPY) [attr-defined]
    Module has no attribute "gru"

        1134  |            result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,
        1135  |                             self.dropout, self.training, self.bidirectional, self.batch_first)
        1136  |        else:
    >>> 1137  |            result = _VF.gru(input, batch_sizes, hx, self._flat_weights, self.bias,
        1138  |                             self.num_layers, self.dropout, self.training, self.bidirectional)
        1139  |        output = result[0]
        1140  |        hidden = result[1]

  Error (MYPY) [attr-defined]
    Module has no attribute "rnn_tanh_cell"

        1273  |            hx = hx.unsqueeze(0) if not is_batched else hx
        1274  |
        1275  |        if self.nonlinearity == "tanh":
    >>> 1276  |            ret = _VF.rnn_tanh_cell(
        1277  |                input, hx,
        1278  |                self.weight_ih, self.weight_hh,
        1279  |                self.bias_ih, self.bias_hh,

  Error (MYPY) [attr-defined]
    Module has no attribute "rnn_relu_cell"

        1279  |                self.bias_ih, self.bias_hh,
        1280  |            )
        1281  |        elif self.nonlinearity == "relu":
    >>> 1282  |            ret = _VF.rnn_relu_cell(
        1283  |                input, hx,
        1284  |                self.weight_ih, self.weight_hh,
        1285  |                self.bias_ih, self.bias_hh,

  Error (MYPY) [attr-defined]
    Module has no attribute "lstm_cell"

        1377  |        else:
        1378  |            hx = (hx[0].unsqueeze(0), hx[1].unsqueeze(0)) if not is_batched else hx
        1379  |
    >>> 1380  |        ret = _VF.lstm_cell(
        1381  |            input, hx,
        1382  |            self.weight_ih, self.weight_hh,
        1383  |            self.bias_ih, self.bias_hh,

  Error (MYPY) [attr-defined]
    Module has no attribute "gru_cell"

        1469  |        else:
        1470  |            hx = hx.unsqueeze(0) if not is_batched else hx
        1471  |
    >>> 1472  |        ret = _VF.gru_cell(
        1473  |            input, hx,
        1474  |            self.weight_ih, self.weight_hh,
        1475  |            self.bias_ih, self.bias_hh,



>>> Lint for torch/onnx/_type_utils.py:

  Error (MYPY) [attr-defined]
    "type[TensorProtoDataType]" has no attribute "FLOAT8E5M2FNUZ"

        368  |    JitScalarType.QINT32: _C_onnx.TensorProtoDataType.INT32,
        369  |    JitScalarType.FLOAT8E5M2: _C_onnx.TensorProtoDataType.FLOAT8E5M2,
        370  |    JitScalarType.FLOAT8E4M3FN: _C_onnx.TensorProtoDataType.FLOAT8E4M3FN,
    >>> 371  |    JitScalarType.FLOAT8E5M2FNUZ: _C_onnx.TensorProtoDataType.FLOAT8E5M2FNUZ,
        372  |    JitScalarType.FLOAT8E4M3FNUZ: _C_onnx.TensorProtoDataType.FLOAT8E4M3FNUZ,
        373  |}
        374  |

  Error (MYPY) [attr-defined]
    "type[TensorProtoDataType]" has no attribute "FLOAT8E4M3FNUZ"

        369  |    JitScalarType.FLOAT8E5M2: _C_onnx.TensorProtoDataType.FLOAT8E5M2,
        370  |    JitScalarType.FLOAT8E4M3FN: _C_onnx.TensorProtoDataType.FLOAT8E4M3FN,
        371  |    JitScalarType.FLOAT8E5M2FNUZ: _C_onnx.TensorProtoDataType.FLOAT8E5M2FNUZ,
    >>> 372  |    JitScalarType.FLOAT8E4M3FNUZ: _C_onnx.TensorProtoDataType.FLOAT8E4M3FNUZ,
        373  |}
        374  |
        375  |_ONNX_TO_SCALAR_TYPE = {v: k for k, v in _SCALAR_TYPE_TO_ONNX.items()}



>>> Lint for torch/package/package_exporter.py:

  Error (MYPY) [arg-type]
    Argument 2 to "write_record" of "PyTorchFileWriter" has incompatible type
    "Storage"; expected "bytes | int"

         941  |                    storage = storage.cpu()
         942  |                num_bytes = storage.nbytes()
         943  |                self.zip_file.write_record(
    >>>  944  |                    f".data/{storage_id}.storage", storage, num_bytes
         945  |                )
         946  |            return ("storage", storage_type, storage_id, location, storage_numel)
         947  |



>>> Lint for torch/sparse/semi_structured.py:

  Error (MYPY) [arg-type]
    Argument 2 to "pad" has incompatible type "tuple[int, Any, int, Any]";
    expected "list[int]"

        282  |        to_pad_m = -m % min_rows if m < min_rows or m % min_rows else 0
        283  |        to_pad_n = -n % min_cols if n < min_cols or n % min_rows else 0
        284  |        if to_pad_m or to_pad_n:
    >>> 285  |            return torch.nn.functional.pad(dense_input, (0, to_pad_n, 0, to_pad_m))
        286  |        else:
        287  |            return dense_input
        288  |

  Error (MYPY) [call-arg]
    Unexpected keyword argument "alg_id" for "_cslt_sparse_mm"

        508  |                f"`{self.__class__.__name__}` matmul: operation is not supported"
        509  |            )
        510  |        else:
    >>> 511  |            res = torch._cslt_sparse_mm(
        512  |                self.packed,
        513  |                B,
        514  |                bias=bias,



>>> Lint for torch/testing/_comparison.py:

  Error (MYPY) [attr-defined]
    Module has no attribute "uint64"

         799  |            # For uint64, this is not sound in general, which is why promote_types doesn't
         800  |            # allow it, but for easy testing, we're unlikely to get confused
         801  |            # by large uint64 overflowing into negative int64
    >>>  802  |            if actual_dtype in [torch.uint64, torch.uint32, torch.uint16]:
         803  |                actual_dtype = torch.int64
         804  |            if expected_dtype in [torch.uint64, torch.uint32, torch.uint16]:
         805  |                expected_dtype = torch.int64

  Error (MYPY) [attr-defined]
    Module has no attribute "uint32"

         799  |            # For uint64, this is not sound in general, which is why promote_types doesn't
         800  |            # allow it, but for easy testing, we're unlikely to get confused
         801  |            # by large uint64 overflowing into negative int64
    >>>  802  |            if actual_dtype in [torch.uint64, torch.uint32, torch.uint16]:
         803  |                actual_dtype = torch.int64
         804  |            if expected_dtype in [torch.uint64, torch.uint32, torch.uint16]:
         805  |                expected_dtype = torch.int64

  Error (MYPY) [attr-defined]
    Module has no attribute "uint16"

         799  |            # For uint64, this is not sound in general, which is why promote_types doesn't
         800  |            # allow it, but for easy testing, we're unlikely to get confused
         801  |            # by large uint64 overflowing into negative int64
    >>>  802  |            if actual_dtype in [torch.uint64, torch.uint32, torch.uint16]:
         803  |                actual_dtype = torch.int64
         804  |            if expected_dtype in [torch.uint64, torch.uint32, torch.uint16]:
         805  |                expected_dtype = torch.int64

  Error (MYPY) [attr-defined]
    Module has no attribute "uint64"

         801  |            # by large uint64 overflowing into negative int64
         802  |            if actual_dtype in [torch.uint64, torch.uint32, torch.uint16]:
         803  |                actual_dtype = torch.int64
    >>>  804  |            if expected_dtype in [torch.uint64, torch.uint32, torch.uint16]:
         805  |                expected_dtype = torch.int64
         806  |            dtype = torch.promote_types(actual_dtype, expected_dtype)
         807  |            actual = actual.to(dtype)

  Error (MYPY) [attr-defined]
    Module has no attribute "uint32"

         801  |            # by large uint64 overflowing into negative int64
         802  |            if actual_dtype in [torch.uint64, torch.uint32, torch.uint16]:
         803  |                actual_dtype = torch.int64
    >>>  804  |            if expected_dtype in [torch.uint64, torch.uint32, torch.uint16]:
         805  |                expected_dtype = torch.int64
         806  |            dtype = torch.promote_types(actual_dtype, expected_dtype)
         807  |            actual = actual.to(dtype)

  Error (MYPY) [attr-defined]
    Module has no attribute "uint16"

         801  |            # by large uint64 overflowing into negative int64
         802  |            if actual_dtype in [torch.uint64, torch.uint32, torch.uint16]:
         803  |                actual_dtype = torch.int64
    >>>  804  |            if expected_dtype in [torch.uint64, torch.uint32, torch.uint16]:
         805  |                expected_dtype = torch.int64
         806  |            dtype = torch.promote_types(actual_dtype, expected_dtype)
         807  |            actual = actual.to(dtype)



>>> Lint for torch/testing/_creation.py:

  Error (MYPY) [attr-defined]
    Module has no attribute "uint16"

         15  |    torch.int16,
         16  |    torch.int32,
         17  |    torch.int64,
    >>>  18  |    torch.uint16,
         19  |    torch.uint32,
         20  |    torch.uint64,
         21  |]

  Error (MYPY) [attr-defined]
    Module has no attribute "uint32"

         16  |    torch.int32,
         17  |    torch.int64,
         18  |    torch.uint16,
    >>>  19  |    torch.uint32,
         20  |    torch.uint64,
         21  |]
         22  |_FLOATING_TYPES = [torch.float16, torch.bfloat16, torch.float32, torch.float64]

  Error (MYPY) [attr-defined]
    Module has no attribute "uint64"

         17  |    torch.int64,
         18  |    torch.uint16,
         19  |    torch.uint32,
    >>>  20  |    torch.uint64,
         21  |]
         22  |_FLOATING_TYPES = [torch.float16, torch.bfloat16, torch.float32, torch.float64]
         23  |_FLOATING_8BIT_TYPES = [



>>> Lint for torch/utils/__init__.py:

  Error (MYPY) [attr-defined]
    Module has no attribute "_swap_tensor_impl"

        65  |            delattr(t2, slot)
        66  |
        67  |    # Swap the at::Tensor they point to
    >>> 68  |    torch._C._swap_tensor_impl(t1, t2)



>>> Lint for torch/utils/_content_store.py:

  Error (MYPY) [arg-type]
    Argument 2 to "pad" has incompatible type "tuple[int, int]"; expected
    "list[int]"

        127  |        # though it could be profitably fused
        128  |        pad = -x.numel() % 4
        129  |        if pad > 0:
    >>> 130  |            x = F.pad(x, (0, pad), "constant", 0)
        131  |        x = x.view(torch.int32)
        132  |        # We run the 32-bit hash five times with differing parameters to
        133  |        # reduce chance of collision



>>> Lint for torch/utils/_python_dispatch.py:

  Error (MYPY) [attr-defined]
    "type[DispatchKey]" has no attribute "PreDispatch"

        160  |
        161  |def _push_mode(mode):
        162  |    k = mode._dispatch_key if hasattr(mode, "_dispatch_key") else None
    >>> 163  |    assert k is None or k == torch._C.DispatchKey.PreDispatch
        164  |    if k is None:
        165  |        _push_on_torch_dispatch_stack(mode)
        166  |        return



>>> Lint for torch/xpu/__init__.py:

  Error (MYPY) [valid-type]
    Variable "torch.xpu._XpuDeviceProperties" is not valid as a type

        234  |    }
        235  |
        236  |
    >>> 237  |def get_device_properties(device: Optional[_device_t] = None) -> _XpuDeviceProperties:
        238  |    r"""Get the properties of a device.
        239  |
        240  |    Args:

  Error (MYPY) [attr-defined]
    Module has no attribute "_has_xpu"

         31  |
         32  |def _is_compiled() -> bool:
         33  |    r"""Return true if compile with XPU support."""
    >>>  34  |    return torch._C._has_xpu
         35  |
         36  |
         37  |if _is_compiled():

  Error (MYPY) [attr-defined]
    Module has no attribute "_XpuDeviceProperties"; maybe
    "_CudaDeviceProperties"?

         35  |
         36  |
         37  |if _is_compiled():
    >>>  38  |    _XpuDeviceProperties = torch._C._XpuDeviceProperties
         39  |    _exchange_device = torch._C._xpu_exchangeDevice
         40  |    _maybe_exchange_device = torch._C._xpu_maybeExchangeDevice
         41  |else:

  Error (MYPY) [attr-defined]
    Module has no attribute "_xpu_exchangeDevice"; maybe
    "_cuda_exchangeDevice"?

         36  |
         37  |if _is_compiled():
         38  |    _XpuDeviceProperties = torch._C._XpuDeviceProperties
    >>>  39  |    _exchange_device = torch._C._xpu_exchangeDevice
         40  |    _maybe_exchange_device = torch._C._xpu_maybeExchangeDevice
         41  |else:
         42  |    # Define dummy if PyTorch was compiled without XPU

  Error (MYPY) [attr-defined]
    Module has no attribute "_xpu_maybeExchangeDevice"; maybe
    "_cuda_maybeExchangeDevice"?

         37  |if _is_compiled():
         38  |    _XpuDeviceProperties = torch._C._XpuDeviceProperties
         39  |    _exchange_device = torch._C._xpu_exchangeDevice
    >>>  40  |    _maybe_exchange_device = torch._C._xpu_maybeExchangeDevice
         41  |else:
         42  |    # Define dummy if PyTorch was compiled without XPU
         43  |    _XpuDeviceProperties = _dummy_type("_XpuDeviceProperties")  # type: ignore[assignment, misc]

  Error (MYPY) [attr-defined]
    Module has no attribute "_xpu_getDeviceCount"; maybe
    "_cuda_getDeviceCount"?

         54  |    r"""Return the number of XPU device available."""
         55  |    if not _is_compiled():
         56  |        return 0
    >>>  57  |    return torch._C._xpu_getDeviceCount()
         58  |
         59  |
         60  |def is_available() -> bool:

  Error (MYPY) [attr-defined]
    Module has no attribute "_xpu_init"; maybe "_rpc_init"?

        114  |        if not _is_compiled():
        115  |            raise AssertionError("Torch not compiled with XPU enabled")
        116  |        # This function inits XPU backend and detects bad fork processing.
    >>> 117  |        torch._C._xpu_init()
        118  |        # Some of the queued calls may reentrantly call _lazy_init(); We need to
        119  |        # just return without initializing in that case.
        120  |        _tls.is_initializing = True

  Error (MYPY) [attr-defined]
    Module has no attribute "_xpu_setDevice"; maybe "_cuda_setDevice" or
    "_cuda_getDevice"?

        196  |    _lazy_init()
        197  |    device = _get_device_index(device)
        198  |    if device >= 0:
    >>> 199  |        torch._C._xpu_setDevice(device)
        200  |
        201  |
        202  |def get_device_name(device: Optional[_device_t] = None) -> str:

  Error (MYPY) [attr-defined]
    _XpuDeviceProperties? has no attribute "name"

        211  |    Returns:
        212  |        str: the name of the device
        213  |    """
    >>> 214  |    return get_device_properties(device).name
        215  |
        216  |
        217  |@lru_cache(None)

  Error (MYPY) [attr-defined]
    Module has no attribute "_xpu_getDevice"; maybe "_cuda_getDevice" or
    "_cuda_setDevice"?

        254  |def current_device() -> int:
        255  |    r"""Return the index of a currently selected device."""
        256  |    _lazy_init()
    >>> 257  |    return torch._C._xpu_getDevice()
        258  |
        259  |
        260  |def _get_device(device: Union[int, str, torch.device]) -> torch.device:

  Error (MYPY) [attr-defined]
    Module has no attribute "_xpu_setStream"; maybe "_cuda_setStream"?

        328  |          device_index (int): selected device index.
        329  |          device_type (int): selected device type.
        330  |    """
    >>> 331  |    torch._C._xpu_setStream(
        332  |        stream_id=stream_id,
        333  |        device_index=device_index,
        334  |        device_type=device_type,

  Error (MYPY) [attr-defined]
    Module has no attribute "_xpu_getCurrentStream"; maybe
    "_cuda_getCurrentStream"?

        364  |            (default).
        365  |    """
        366  |    _lazy_init()
    >>> 367  |    streamdata = torch._C._xpu_getCurrentStream(
        368  |        _get_device_index(device, optional=True)
        369  |    )
        370  |    return Stream(

  Error (MYPY) [attr-defined]
    Module has no attribute "_xpu_synchronize"; maybe "_cuda_synchronize"?

        382  |    """
        383  |    _lazy_init()
        384  |    device = _get_device_index(device, optional=True)
    >>> 385  |    return torch._C._xpu_synchronize(device)
        386  |
        387  |
        388  |def empty_cache() -> None:

  Error (MYPY) [attr-defined]
    Module has no attribute "_xpu_emptyCache"; maybe "_mps_emptyCache" or
    "_cuda_emptyCache"?

        395  |        of XPU memory in certain cases.
        396  |    """
        397  |    if is_initialized():
    >>> 398  |        torch._C._xpu_emptyCache()
        399  |
        400  |
        401  |def _get_generator(device: torch.device) -> torch._C.Generator:



>>> Lint for torch/xpu/streams.py:

  Error (MYPY) [name-defined]
    Name "torch._C._XpuStreamBase" is not defined

         11  |    torch._C.__dict__["_XpuEventBase"] = _dummy_type("_XpuEventBase")
         12  |
         13  |
    >>>  14  |class Stream(torch._C._XpuStreamBase, _StreamBase):
         15  |    r"""Wrapper around a XPU stream.
         16  |
         17  |    A XPU stream is a linear sequence of execution that belongs to a specific

  Error (MYPY) [name-defined]
    Name "torch._C._XpuEventBase" is not defined

         96  |        return f"torch.xpu.Stream(device={self.device} sycl_queue={self.sycl_queue:#x})"
         97  |
         98  |
    >>>  99  |class Event(torch._C._XpuEventBase, _EventBase):
        100  |    r"""Wrapper around a XPU event.
        101  |
        102  |    XPU events are synchronization markers that can be used to monitor the

